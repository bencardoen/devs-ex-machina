\documentclass[8pt,a4paper]{report}
\usepackage{hyperref}
\usepackage[latin1]{inputenc}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beramono}
\newcommand\Small{\fontsize{9}{9.2}\selectfont}
\newcommand*\LSTfont{\Small\ttfamily\SetTracking{encoding=*}{-60}\lsstyle}
\usepackage{url}
  \lstset{basicstyle=\ttfamily\tiny\color{blue}}
\usepackage{graphicx}
%\usepackage[]{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}

\title{Project Report\\\textsc{DEVS Ex Machina}}
\author{Ben Cardoen\\Pieter Lauwers\\Stijn Manhaeve\\Tim Tuijn\\Matthijs Van Os}
\date{}
\begin{document}
\maketitle
\tableofcontents{}

%
\chapter{Summary}
%
\section*{}
The project implements a C++ port of the PyPDEVS project, extended with conservative synchronization and a change in system architecture from distributed simulation on several nodes (asymmetric mp) to a symmetric multiprocessing architecture. \\The summarized goal of the project is to provide an implementation of the offered DEVS formalisms with significantly lower execution time by leveraging the smp architecture.

\section*{Terminology}
The literature sometimes exchanges terms, to avoid any confusion the following frequent terms are listed with their associated meaning as used in this document:
\begin{itemize}
  \item Core : alias kernel. Single instance, containing a set of models for which it is responsible, exchanges messages generated by those models and keeps time state. Can correspond with Logical Process in terms of what it represents.
  \item Logical Process : set of models with strong internal coherence.
  \item Distributed parallelism : computation distributed over several physically separated machines communicating by means of a network stack.
  \item SMP : symmetric multiprocessing, running a parallel computation on several distinct CPU cores/threads on the same machine with shared memory (and/or cache).
  \item message : any exchange of high level information between entities, user defined or for inter-kernel synchronization.
  \item lookahead : Timespan relative to current state where the output of the model is not influenced by its input. (as per definition in \cite{cons}, app B)
  \item eot : Earliest output time. Timepoint in future where the core/kernel will output events.
  \item eit : Earliest input time. Timepoint in future where the core/kernel expects to
  be change state on receiving event.
\end{itemize}
Lookahead is collected per model per kernel, eit/eot are based on information from models but determined on a core/kernel level.

\chapter{Architecture Analysis}
\section{Implementation language}
Python is a very powerful language, but due to its interpreted nature it suffers from a performance point of view. Python's GIL \cite{gil} makes this issue even more serious whenever multi-threading is required. Although it relieves the developer from tracking resource allocation, this comes at a price in the overhead caused by the GC. \\
In contrast, C++ offers low-level powerful thread primitives, but requires the developer to have a very deep understanding of parallel programming techniques. By using RAII, a C++ developer can leverage safe resource (de)allocation without losing performance.\\
A significant advantage Python offers a developer is relative cross-platform capability. This is non-trivial for a C++ developer, since at the very least the toolchains will be radically different in usage, and sometimes even in implementation of the C++ standard. The edit-build-test cycle is also non-trivial for a large C++ application, whereas in Python this is negligible. In contrast, Python does not offer the programmer any of the C++ compiler's many compile time checks (static typechecks and program analysis).

\section{Operating system support}
The project requires a compliant C++11 compiler and library, but beyond that is not really bound to special features. For testing, GTest is a requirement, and the only two other dependencies are the ubiquitous Boost and Cereal, the latter is provided with the project as it is header only.

\section{Object representation}
In Python everything is a reference, and since the DEVS formalisms require that the user be allowed to override most of the basic entities, the best representation for an object was a shared\textunderscore ptr. Passing by value could have been safer (e.g. between threads) but would introduce slicing.\\ Another advantage was that we would use as little memory as possible, for example sending a message means passing a const reference to a shared pointer, which is usually equal in size to 2 pointers. By passing by const ref we minimize most of the penalty usually involved in copyconstructing shared pointers. \\
The only downside is the synchronized access to the reference count in the shared pointer. There is also a need for more boilerplate code to use shared pointers in containers (providing pass-through hash function, comparison etc). 

\section{Messaging Size}
A message is constructed once, and after that never copied. Messages are sent and received by reference to smart pointer, incurring a size cost of (on 64-bit platforms) 128bit. The payload (and its size) is irrelevant to the simulation up to the point where the message is actually received. A default implementation allows the payload stored as a std::string, but a specialized message class can be used for sending objects of a different type. When constructing a message by using the functionality provided by the Port class, this happens automatically. When receiving messages, a free function template is provided to extract the object from the message.\\
For example:
\begin{lstlisting}[language=C++,basicstyle=\small\color{black}]
//initialization of necessary objects is left out.
double value;
n_model::t_portptr port;

//create messages that contain a double
auto msgVector = port->createMessages(value);

//say we receive this message somewhere else
n_network::t_msgptr received =  msgVector[0];

//retrieve the data when receiving the message
const double& data = n_network::getMsgPayload<double>(received);

//Assuming that no Z function was applied to the message.
assert(value == data);
\end{lstlisting}
\paragraph{Note} While it is technically possible, we strongly discourage the usage of the following data types in these messages:
\begin{itemize}
	\item raw pointers to heap-allocated object.\\
			Messages can be destroyed during a time warp prior to being delivered to a model. This will result in a memory leak if there are no other references to these objects.
	\item pointers or references to local objects.\\
		When these objects go out of scope, they are destructed, rendering the pointer or reference invalid.
\end{itemize}

\chapter{Design}
\section{Overview}
We've slightly departed from the PyPDEVS structure, to allow for more encapsulation and decoupling of the core entities. This clear separation of data and responsibilities between classes made the threading part of the project easier (no or minimal shared state). See the \hyperref[simpledesign]{Simple Design figure \ref*{simpledesign}}.\\

\begin{figure}[h!]
	\makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{Design.png}}
	\caption{Simple Design}
	\label{simpledesign}
\end{figure}

\section{Translating Dynamic Typing to Static Typing}
Certain classes of the simulation, such as the messages between models and the model states, must be able to contain objects of any type. However, C++ is a statically typed language. To overcome this problem, we came up with the following three solutions:
\begin{itemize}
	\item Keep a \texttt{void*} that points to the actual data. This approach is very fragile and impossible to serialize. Misuse of \texttt{void*} could result in anything from undefined behaviour to stack corruption.
	\item Serialize everything to \texttt{std::string}. Serialization is not a trivial operation. Continuously serializing and deserializing can be a very large performance hit. Moreover, it would require each and every type to be serializable. On the flip side, the compiler can help by detecting whether or not the object can be serialized or not.
	\item Make use of inheritance and create a specialized subclass for each and every datatype. In the case of the messages, a class template can be used to facilitate this process. In the case of the model states, we decided that it is best that the user provides the subclass.\\
	Since we use pointers for messages and states, a pointer cast is all that is needed to access the members of the specialized class. For extra safety, we suggest to use \texttt{std::dynamic\_pointer\_cast}.
\end{itemize}
We chose the third option because it both offers speed and safety.

\section{Class decomposition}
The active entities in the project were discovered using several methods: use cases, argumentation and counterargumentation during the weekly meetings, and studying the existing implementation. This led to the following simplified decomposition:
\begin{enumerate}
\item Model : provides DEVS formalism implementation of an Coupled/Atomic model.
\item Core : drives transitions, exchanges messages, synchronizes
\item Controller : Constructs simulator, controls simulation
\item Tracer : Traces output
\end{enumerate}
The other classes bind/create instances of the above (e.g. LocationTable, Allocator, Factory), or provide essential functionality (e.g. Message, Network, Scheduler)\\
For a full overview, please consult the documentation generated by doxygen.

\section{Network -- Kernel communication}
PyPDEVS uses MPI to exchange messages between kernels, whereas we could use shared memory to communicate between kernels (each running on 1 or more threads). We created a network architecture mimicking the approach used by PyPDEVS, without sacrificing the speed and ordering advantages of shared memory. The Network class consists of a single locked receiving port, and has for each existing kernel an outgoing queue. Receiving a message locks the network for the time it needs to place it on an integer indexed queue (very fast). If a kernel wants to receive messages it can do so if/when it is ready to do so, this only locks if at the same time a message is being received by the network.\\ Several kernels can pull messages without contention in parallel from the network (since the output queues are not shared). \\This is illustrated in \hyperref[networkdesign]{Network design figure \ref*{networkdesign}}.

\begin{figure}[h!]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth]{network.png}}
	\caption{Network cooperation diagram}
	\label{networkdesign}
\end{figure} 
Early benchmarks showed a repeatable throughput of 5e5 msg/s, with 4 kernels on modest hardware. The problems of (unintentional) out of order receipt and potential loss of messages are a non-issue in this context, since we use FIFO queues and loss of a message would implicate severe system failure (in which case message loss is the lesser problem).\\
Another advantage of hiding the message exchange behind a shared Network class instance is that, should this be required in the future, the kernel could run on different machines without requiring much redesigning of the system. An RPC implementation would simply have to implement the Network class on both sides.

\section{Tracing}
The tracing uses template programming, and is largely asynchronous w.r.t. the simulator kernels. This can be seen by the high utilization values when benchmarking the examples.
The tracing component makes heavy use of template programming. For more detailed information, please refer to \href{run:./tracersReport.pdf}{tracersReport.pdf}.

\section{Models}
The models hide all implementation details for the particular DEVS formalisms, without limiting the user to extend into any subclassed model of his/her choice. The use can focus on what he/she wants the model to emulate, and not worry about for example locking, allocation etc. We refer to the manual for more information about the creation of these models.

\section{Scheduler}
A Scheduler was implemented based on the prototyping code. As required, as list based scheduler was implemented. \\
The interface from PyPDevs' scheduler class was reused, but internally the scheduler class in our project differs on a few points. \\
\subsection{Composition}
All schedulers are based on a heap, which order items to be scheduled. We never actually schedule models, only entries (A struct containing name and time to schedule), allowing for far faster operation. This is complemented by a hashmap, so that entries can have O(1) lookup and erasure (required for the use case of antimessaging, among others). The operation massReschedule() was left out of the interface, since it can be easily composed using erase() and push\textunderscore back().
\\
\subsection{Implementation}
The actual heap used is either based on std::list as required, or one of Boost's heaps. The user is provided with a convenience factory function that creates Scheduler instances with varying heap backends. In general, the Fibonacci heap is one of the fastest, with skew heap sometimes coming out ahead. The list based heap is always a factor 2 slower, though again this depends on the usage scenario. \\Important to note is that only node based heaps can be used efficiently (without compromising the performance of erase() and contains()). We store on insertion an iterator with the key of the pushed element in the hashmap. This also works as a safety check, preventing double insertion (depending ofcourse on the hash-specialization semantics). This freedom of implementation is visible in the tracers where duplicate elements are allowed if they have a different address, contrasted with the model scheduler which disallows a model to be scheduled twice.\\
Finally, the std::hash$<>$ specialization used by the message scheduler allows O(1) annihilation of a message by its antimessage (since the hash is identical).
\subsection{Differences}
We intended to use the scheduler class not only for scheduling models, but also to store pending incoming messages, and tracing messages. To allow this, the Scheduler class only requires from a scheduled item that it has operator$<$ and std::hash$<$T$>$ defined.

\section{Controller}
For the controller, care was taken in making the interface both functional and minimal. The default configuration is quick to set up, and customization is possible should the user require it.

\section{Serialization}
For serialization we used the Cereal \cite{cereal} header-only framework. We opted for separate load and save functions for full flexibility. For each type that occurs in the form of a pointer we implemented a static load and construct method since the object first has to be created before the saved data can be imported. Cereal has a small coupling with the code (models) that has to be written by the user as the user has to register his own classes due to polymorphism.

All the main functionality to load objects is implemented in the serialize function itself. Most classes also have an implementation for load and construct so that objects can separately be constructed before filling it with the saved data. This is necessary in cases where pointers to objects of those classes have to be serialized or where there is no default constructor available. The implementation of those load and construct methods is always restricted to creating an object with - most of the time - meaningless temporary datamembers after which the serialize function of the created object is called to load the actual data.

Since Cygwin is not compatible with the std::to\_string() function, which is used for serialization to human readable formats like JSON or XML, only serialization to binary files is supported. Even in the binary archive implementation of Cereal the std::to\_string() function occurred, though only on non-decisive places, resulting in some slightly modified headers for this project.  The binary archives are lighter and give more speed than the user-readable formats but comes with the disadvantage of not providing full garanty that saved data can be correctly loaded on another architecture. 

Since Cereal was hard to use and gave a lot of undesired errors while serializing const datamembers and objects of purely virtual classes, we decided to violate some proper design rules to overcome this.

As we have experienced problems with an unressolved bug from Cereal - covering the errors in case of specific nested circular smart pointers - we were forced to not always follow the elegant design of Cereal and explicit force the calling of some serialization function in the load and construct functions.

\chapter{Non-functional requirements}
\section{Correctness}
\subsubsection{Testing}
The use of the jenkins CI was heavily leveraged, in combination with extensive testing. Any feature increment could only be merged into the project if it passed the developer's own jenkins instance.
\subsubsection{Thread races and deadlock}
Deadlocks and race conditions are very hard to debug, let alone prevent. To help us in this area as much as possible we used prototypes of threaded code in the tests before merging them into the main classes, and each build is checked with ThreadSanitizer \cite{tsan} which proved invaluable in preventing races and deadlocks in the threaded codepaths.
\subsubsection{Memory leaks and undefined behaviour}
With most entities represented as smart pointer, the need to ensure a leak free simulator nevertheless remained. To check for allocation misuse and a whole other series of errors we used Valgrind \cite{vg}. This meant for example that we detected a leak caused by circular referencing smart pointers.

\subsubsection{Teamwork}
We met at least once every week (barring the exam period), with each meeting having a formal report. Whenever required, teammembers used video conferencing to solve critical bugs or discuss design/coding issues.
\subsubsection{Code review}
Although there was no real formal code review done up to now, we've had several debug sessions reviewing each others code. This means the most critical used sections of the code are allready reasonably well reviewed, albeit not formally.
\subsubsection{Portability}
We develop on Unix, Windows and MacOSX.

In Windows we chose the Cygwin platform instead of MinGW to compile our project on. This because we make use of the C++11 std::thread objects; MinGW doesn't support POSIX threads. When building the project with GTest on Windows you need to use the -std=gnu++11 flag instead of the standard -std=c++11 flag, because we need the GNU extensions to correctly link everything.

\chapter{Functional requirements}
\section{DEVS Formalisms}
\subsection{Single core}
The single core implementations follow exactly the runtime behaviour shown by the PyPDEVS simulator. We've chosen to emulate the Select() function with an integer priority value the user can set. This saves 1 virtual function call per transition and can quite easily be optimized by the compiler since it remains constant.
\subsubsection{DirectConnect}
The directConnect algorithm has gone relatively unchanged from its implementation in PyPDEVS, though the role of the RootDEVS class has been simplified somewhat. Its main function has been reduced to executing directConnect and then storing the resulting list of connected atomic models.
\subsubsection{Dynamic Structured}
Dynamic structured DEVS is completely supported in single core simulation. This includes allowing the following structural changes during the simulation:
\begin{itemize}
	\item adding/removing connections
	\item adding/removing ports
	\item adding/removing submodels.
\end{itemize}
These changes are only allowed during a special phase of the simulation. The simulator provides a map object of type \lstinline[language=C++,basicstyle=\small\color{black}]{std::multimap<std::string, std::string>} that can be used for communication and cooperation between the models. During one simulation step, this communication always happens from bottom to top. Only models that had an internal transition are considered for performing dynamic structured.\\
Unlike the PyPDEVS, we will try to keep a removed model intact when it is removed from the simulation.
\subsection{Parallel}
\subsubsection{Optimistic}
Optimistic time synchronization requires an implementation of timewarp, and more extensive locking than would be required for conservative. The GVT algorithm (Mattern \cite{Mattern}) runs on a separate thread from the simulation kernel threads, so care must be taken that they don't deadlock, but don't race either.
\subsubsection{Conservative}
The implementation of the conservative formalism follows the algorithm as described in \cite{cons}. \\
\paragraph{Algorithm}
The following listing briefly states the steps required in the conservative synchronization algorithm.\\
Initialize eit,eot as (0,0) per kernel.\\
A Time object has an absolute first field , followed by a causal second field.\\
\begin{algorithm}
\caption{Conservative PDEVS}
\label{CHalgorithm}
\begin{algorithmic}
\Procedure{CPDEVS Modified}{}
\State 0.1 Construct a depency graph, determining which models influence this kernel directly
\State 0.2 If a msg is sent : eot = (timestamp > eot) ? timestamp : eot
\While{Termination trigger not reached}
\State 1. Simulate until vtime $>=$ eit
\State 1.a If msg is sent during sim : eot = max(timestamp, eot)
\State 2.a Time x = (eit + min lookahead, 0)
\State 2.b Time y = (msg sent) ? (eit,1) : (time next transition)
\State 2.c Time eot = std::min(x,y)
\State 2.d Share eot value with other kernels.
\State 3   Time eit = min ( $eot_i$ for i in influencing kernels)
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}\\
\subparagraph{Differences with ADEVS}
A big difference between ADEVS' and our implementation is that we hold the eot-values of all kernels in shared memory. This means that step 1.a , which is normally done by the receiver when it inspects the message queue, can be done immediately. This also avoids the queueing delay incurred by simulation traffic.\\
Using the shared memory approach, we also avoid the sending of any synchronization message to other cores (required in ADEVS). \\In step 2.d we simply overwrite our old EOT value with the newly calculated. The shared EOT vector is only written at 1 position for each kernel , and only the influencing EOT values are ever read. \\
Also important to note is that there exist several edge cases not listed in this algorithm, but force the implementation to do more work than listed above. Consider a kernel with a non-dependent model A (that receives no input). By definition A has eit = oo, causing all eot-calculation to be skipped forever. We can't do this, if there exists at least 1 model depending on A, this model would get incorrect eot-values (either 0 or oo, which neither are correct since A is not prohibited from sending output).\\
The correct behaviour here is to replace eit with the vtime of the kernel at each simulation step and to use that in the algorithm.\\
 Furthermore, collecting lookahead values from models needs to be done in step 1, a lookahead value is calculated starting from the time it acquires a new state. This means that the lookahead value used in 2.a has to be continuously updated, regardless of usage, and has to be reset between simulation rounds (step 1 can entail several of such rounds). If we don't reset the minimum lookahead value in between rounds, there are scenarios where an incorrect minimum of a model that is no longer scheduled in the next round can warp the calculation. \\ 
 The calculation done in steps 2, 3 are quite efficiently implemented, although a future optimization could delay step 3 until several rounds have passed, more in line with the theoretical algorithm.
\subparagraph{Dependency calculation}
The author of the models needs to supply the kernel, for each model, with a list of models by whom it is influenced. Receiving a message that alters a model's output is defined as influencing in this context. Using the allocation information provided by the Controller, the Kernel will then determine which other Kernels (including possibly itself) it is dependent on.\\
It is important to note that the user need not interpret this dependency relation as transition, and needs to look only at 1-edge removed models. \\
To clarify this last point: suppose we have 3 models A, B, C with A sending output to B, and B to C. C is indirectly influenced by A, but need not record this information (which is non trivial to retrieve for a non-trivial model, disregarding cycles for the moment). \\
Let A be allocated @ kernel 0, B @ 1, C @ 2. While the dependency is transitive, since kernel 1 is already bound by kernel 0, kernel 2 does not require kernel 0's information (eot). By moving this transitivity to the kernel level, the model's author only has to look at 1-edge distant dependencies. \\
At construction a Conservative Kernel will collect , for each model it holds, the list of names of influencing models. Next, the kernel requests for each unique name the allocation information (O(1) lookup). Note that this can be optimized if the model stores its allocation information, but this then prohibits runtime reallocation should this be implemented in the future.

\chapter{Obstacles}
\section{Virtualization}
Virtualization proved a significant hurdle by exposing potential faults. Standard compliant code was found to be leaking memory by not calling container member destructors but only on a VM with Linux as Guest and Windows as Host. This was never reproducible in another setting.
\section{Threading}
\subsection{String}
libstdc++'s std::string implementation appears to be still Copy On Write (despite 21.4.1 p6 from the C++ standard) \cite{cpp}. This triggers an alert by threadsanitizer for a possible race whenever a string was supposedly copied but internally still stored as a reference to the same object. Explicitly invoking the string constructor with the char[] of the original string, proved to be a safe workaround here. Note that this problem is a known issue \cite{cow}, which is solved in GCC 5.1.\\
\subsection{Timing}
We also encountered several scenarios where thread scheduling complicated our kernel control code. An extreme example of this is Valgrind, which can cause multithreaded code to be executed sequentially. This can be fatal for a parallel simulator if a dependent core is scheduled before the influencing core is ever executed. Modifying valgrind's invocation can alleviate this, but there remain edge cases where a core/kernel can get into a state where it can no longer advance (depending on the model composition). The simulator detects this and (eventually) halts the simulation. \\ Note that the above problem is not limited to valgrind, on Virtualized system this is more easily triggered than in a non virtualized setting.\\
Furthermore, we also force an idle kernel (a kernel that has either reaches termination time, or can't advance time (no messages and no scheduled models) to yield/sleep for a short duration. The reason for this behaviour is to allow the other cores execution time, which would otherwise be spent in idling loops. A condition variable was evaluated as a solution, but not implemented since it also suffered from timing issues (time between thread awakening and core simulating).
\section{Heaps}
2 of the Heaps we use in the Scheduler proved to have subtle corrupting bugs, which were only found quite late in the project. Boost's binomial heap, for reasons so far unknown, sometimes fails to deallocate/remove nodes/items. The actual failure is masked until the next insertion, where we then triggered one of our own invariant checks (the heap size was no longer equal to that of the hashmap). It should be noted that we already experienced similar issues with the binomial heap not deallocating memory despite size() returning 0. \\ Time prevented us from writing up a formal bug report, since the bug is hard to reproduce and even harder to detect.
The D-ary heap triggered an move-assing to self assertion when we used the Debugging version of the STL, so we had to remove both from our Schedulers. This is no way impacted performance, as the Fibonacci heap was in practice faster (with the skew heap sometimes overtaking it).
\section{Toolchains}
To develop we used the required toolchains:\\ g++/clang++ as compiler, Eclipse+CDT as IDE, git for dvcs, jenkins as ci, cmake as build tool supplemented with some shell and python scripting.
The plot plugin from Jenkins was used to do perfomance regression testing, but the plugin itself is not that stable. Nonetheless, we detected several random regression using the plotted graphs. \\
G++ failed with an internal compiler error in certain multi-build cmake scenario's, for which a  bug was filed \cite{gpp}. GDB also intermittently failed with a segmentation fault when debugging threaded code. Using Fedora's bug reporting mechanism abrt a bug \cite{gdb} was filed for this as well.
\section{Logger}
The recommended logger (g3log), while powerful and fast, proved a dead end since it does not compile on Cygwin. 
Stijn therefore wrote his own implementation.\\
This implementation only covers the most basic needs:
\begin{itemize}
	\item asynchronous output
	\item thread safe
	\item multiple logging levels currently, the following 4 levels are supported:\\
		\emph{INFO}, \emph{DEBUG}, \emph{WARNING} and \emph{ERROR}
	\item levels can be individually switched on/off at compile time
	\item No logging code is executed whatsoever if the logger is disabled.
	\item log messages contain file name and line number of the log command.
	\item basic crash safety. The logger will catch the following signals and flush all remaining output before exiting the program:
		\begin{itemize}
			\item \emph{SIGSEGV} Segmentation fault, sent by the operating system.
			\item \emph{SIGTERM} Termination request, sent by the program.
			\item \emph{SIGABRT} Abort --- abnormal termination, e.g. when an assertion fails.
			\item \emph{SIGINT} Interrupt, used for catching crtl+C, which is useful for debugging deadlocks.
		\end{itemize}
\end{itemize}

\paragraph{Asynchronous output}
To implement the asynchronous output, we made use of a custom stream buffer. The original design is by Dietmar K{\"u}hl \cite{asynchwrite}. We modified this design in order to make it thread safe.

\chapter{Performance analysis}
To measure the performance of \textsc{DEVS Ex Machina} under more demanding conditions, \textbf{DEVStone} and \textbf{PHOLD} benchmarks were used in combination with the \texttt{perf} profiler tool. Every time we compare the performance of our implementation with \textit{PythonPDEVS}.\\
Note: at the moment, no major research has gone into the use of compiler optimizations yet. Potentially, there is still speed to be gained here.\\
\\
{\footnotesize[All the following tests were performed using an \textit{Intel Core i5 M520} CPU at 2.40GHz, parallel testing was done using 2 simulation cores.]}


\section{Classic DEVS}
Using Classic DEVS we see noticeable speedups compared to PyPDEVS. Both tests are completed in far less time, using the CPU more efficiently.

\subsection{DEVStone}
The DEVStone benchmark used for both implementations has a width of 2 and a depth of 3.
\paragraph{\textsc{DEVS Ex Machina}}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_devstone classic 2 3

 Performance counter stats for './dxexmachina_devstone classic 2 3':

        174.007707 task-clock (msec)         #    1.111 CPUs utilized          
               184 context-switches          #    0.001 M/sec                  
               212 cpu-migrations            #    0.001 M/sec                  
               696 page-faults               #    0.004 M/sec                  
       463,684,386 cycles                    #    2.665 GHz                     [83.13%]
       196,709,921 stalled-cycles-frontend   #   42.42% frontend cycles idle    [85.77%]
       103,770,146 stalled-cycles-backend    #   22.38% backend  cycles idle    [68.62%]
       557,355,313 instructions              #    1.20  insns per cycle        
                                             #    0.35  stalled cycles per insn [86.11%]
       131,411,927 branches                  #  755.208 M/sec                   [85.62%]
         1,720,031 branch-misses             #    1.31% of all branches         [83.38%]

       0.156664362 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{PythonPDEVS}}
\begin{Verbatim}[fontsize=\small]
$ perf stat python experiment.py default 2 false > out.txt

 Performance counter stats for 'python experiment.py default 2 false':

        709.776558 task-clock (msec)         #    0.989 CPUs utilized          
               191 context-switches          #    0.269 K/sec                  
                34 cpu-migrations            #    0.048 K/sec                  
             2,942 page-faults               #    0.004 M/sec                  
     1,687,298,167 cycles                    #    2.377 GHz                     [82.71%]
       866,759,527 stalled-cycles-frontend   #   51.37% frontend cycles idle    [83.25%]
       562,247,464 stalled-cycles-backend    #   33.32% backend  cycles idle    [67.62%]
     1,672,919,036 instructions              #    0.99  insns per cycle        
                                             #    0.52  stalled cycles per insn [83.82%]
       375,443,818 branches                  #  528.961 M/sec                   [83.25%]
        14,867,814 branch-misses             #    3.96% of all branches         [83.28%]

       0.717576473 seconds time elapsed
\end{Verbatim}
\subsection{PHOLD}
\paragraph{\textsc{DEVS Ex Machina}}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_phold classic 1 10 0 10

 Performance counter stats for './dxexmachina_phold classic 1 10 0 10':

         15.847971 task-clock (msec)         #    1.047 CPUs utilized          
                15 context-switches          #    0.946 K/sec                  
                28 cpu-migrations            #    0.002 M/sec                  
               710 page-faults               #    0.045 M/sec                  
        27,365,192 cycles                    #    1.727 GHz                     [74.72%]
        12,605,461 stalled-cycles-frontend   #   46.06% frontend cycles idle    [73.78%]
         7,424,569 stalled-cycles-backend    #   27.13% backend  cycles idle    [86.04%]
        31,095,649 instructions              #    1.14  insns per cycle        
                                             #    0.41  stalled cycles per insn
         6,955,945 branches                  #  438.917 M/sec                  
           132,996 branch-misses             #    1.91% of all branches         [74.99%]

       0.015131930 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{PythonPDEVS}}
\begin{Verbatim}[fontsize=\small]
$ perf stat python experiment.py default 2 false > out.txt

 Performance counter stats for 'python experiment.py default 2 false':

        709.776558 task-clock (msec)         #    0.989 CPUs utilized          
               191 context-switches          #    0.269 K/sec                  
                34 cpu-migrations            #    0.048 K/sec                  
             2,942 page-faults               #    0.004 M/sec                  
     1,687,298,167 cycles                    #    2.377 GHz                     [82.71%]
       866,759,527 stalled-cycles-frontend   #   51.37% frontend cycles idle    [83.25%]
       562,247,464 stalled-cycles-backend    #   33.32% backend  cycles idle    [67.62%]
     1,672,919,036 instructions              #    0.99  insns per cycle        
                                             #    0.52  stalled cycles per insn [83.82%]
       375,443,818 branches                  #  528.961 M/sec                   [83.25%]
        14,867,814 branch-misses             #    3.96% of all branches         [83.28%]

       0.717576473 seconds time elapsed
\end{Verbatim}

\section{Parallel DEVS}
Also in parallel simulations \textsc{DEVS Ex Machina} completes quite a bit quicker, again achieving higher CPU utilization.
Additionally we can see that Conservative PDEVS performs consistently significantly faster than Parallel PDEVS, in the case of PHOLD.

\subsection{DEVStone}
\paragraph{\textsc{DEVS Ex Machina} - Optimistic}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_devstone opdevs 2 2 3

 Performance counter stats for './dxexmachina_devstone opdevs 2 2 3':

         95.265846 task-clock (msec)         #    0.499 CPUs utilized          
               191 context-switches          #    0.002 M/sec                  
                15 cpu-migrations            #    0.157 K/sec                  
             1,229 page-faults               #    0.013 M/sec                  
       197,702,110 cycles                    #    2.075 GHz                     [82.53%]
        95,414,988 stalled-cycles-frontend   #   48.26% frontend cycles idle    [83.54%]
        47,283,951 stalled-cycles-backend    #   23.92% backend  cycles idle    [76.71%]
       199,327,993 instructions              #    1.01  insns per cycle        
                                             #    0.48  stalled cycles per insn [91.61%]
        47,887,986 branches                  #  502.677 M/sec                   [86.14%]
           931,688 branch-misses             #    1.95% of all branches         [75.36%]

       0.191066656 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{DEVS Ex Machina} - Conservative}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_devstone cpdevs 2 2 3

 Performance counter stats for './dxexmachina_devstone cpdevs 2 2 3':

        129.292299 task-clock (msec)         #    0.555 CPUs utilized          
               672 context-switches          #    0.005 M/sec                  
                15 cpu-migrations            #    0.116 K/sec                  
             2,177 page-faults               #    0.017 M/sec                  
       308,983,665 cycles                    #    2.390 GHz                     [74.75%]
       151,616,998 stalled-cycles-frontend   #   49.07% frontend cycles idle    [84.43%]
        82,891,133 stalled-cycles-backend    #   26.83% backend  cycles idle    [76.81%]
       276,296,034 instructions              #    0.89  insns per cycle        
                                             #    0.55  stalled cycles per insn [88.68%]
        66,513,459 branches                  #  514.443 M/sec                   [87.86%]
         1,390,075 branch-misses             #    2.09% of all branches         [77.49%]

       0.233046274 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{PythonPDEVS}}
\begin{Verbatim}[fontsize=\small]
$ perf stat mpirun -np 2 python experiment.py default 2 false > out.txt

 Performance counter stats for 'mpirun -np 2 python experiment.py default 2 false':

        787.125992 task-clock (msec)         #    0.371 CPUs utilized          
             1,120 context-switches          #    0.001 M/sec                  
                91 cpu-migrations            #    0.116 K/sec                  
             8,684 page-faults               #    0.011 M/sec                  
     1,786,217,349 cycles                    #    2.269 GHz                     [83.85%]
       935,480,076 stalled-cycles-frontend   #   52.37% frontend cycles idle    [84.18%]
       619,366,137 stalled-cycles-backend    #   34.67% backend  cycles idle    [65.26%]
     1,796,463,414 instructions              #    1.01  insns per cycle        
                                             #    0.52  stalled cycles per insn [84.16%]
       403,042,819 branches                  #  512.044 M/sec                   [84.70%]
        13,210,328 branch-misses             #    3.28% of all branches         [82.68%]

       2.123888919 seconds time elapsed
\end{Verbatim}

\subsection{PHOLD}
\paragraph{\textsc{DEVS Ex Machina} - Optimistic}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_phold opdevs 2 1 10 0 10

 Performance counter stats for './dxexmachina_phold opdevs 2 1 10 0 10':

         20.741980 task-clock (msec)         #    0.092 CPUs utilized          
                73 context-switches          #    0.004 M/sec                  
                 8 cpu-migrations            #    0.386 K/sec                  
               760 page-faults               #    0.037 M/sec                  
        28,281,718 cycles                    #    1.364 GHz                     [42.03%]
        14,899,039 stalled-cycles-frontend   #   52.68% frontend cycles idle    [79.62%]
         8,373,668 stalled-cycles-backend    #   29.61% backend  cycles idle   
        29,601,476 instructions              #    1.05  insns per cycle        
                                             #    0.50  stalled cycles per insn
         6,754,172 branches                  #  325.628 M/sec                  
           135,220 branch-misses             #    2.00% of all branches         [81.22%]

       0.225834153 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{DEVS Ex Machina} - Conservative}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_phold cpdevs 2 1 10 0 10

 Performance counter stats for './dxexmachina_phold cpdevs 2 1 10 0 10':

         28.509489 task-clock (msec)         #    0.286 CPUs utilized          
                95 context-switches          #    0.003 M/sec                  
                11 cpu-migrations            #    0.386 K/sec                  
               813 page-faults               #    0.029 M/sec                  
        54,757,291 cycles                    #    1.921 GHz                     [63.45%]
        29,057,227 stalled-cycles-frontend   #   53.07% frontend cycles idle    [86.87%]
        15,602,463 stalled-cycles-backend    #   28.49% backend  cycles idle    [90.08%]
        53,326,834 instructions              #    0.97  insns per cycle        
                                             #    0.54  stalled cycles per insn
        12,449,801 branches                  #  436.690 M/sec                  
           219,155 branch-misses             #    1.76% of all branches         [64.68%]

       0.099591165 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{PythonPDEVS}}
PHOLD in parallel on PyPDEVS crashed immediately with an MPI error, hence a comparison was not possible for now.

\section{Compared to \textsc{ADEVS}}
Since the DEVStone benchmark for \textsc{ADEVS} does not support different depths, a test with greater width was performed instead. As we can see, \textsc{ADEVS} is still quite a bit faster.
\paragraph{\textsc{DEVS Ex Machina} - Classic DEVS}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_devstone classic 32 0

 Performance counter stats for './dxexmachina_devstone classic 32 0':

        800.259000 task-clock (msec)         #    1.081 CPUs utilized          
             2,178 context-switches          #    0.003 M/sec                  
               217 cpu-migrations            #    0.271 K/sec                  
               767 page-faults               #    0.958 K/sec                  
     2,199,059,849 cycles                    #    2.748 GHz                     [83.12%]
       862,325,341 stalled-cycles-frontend   #   39.21% frontend cycles idle    [83.36%]
       432,658,584 stalled-cycles-backend    #   19.67% backend  cycles idle    [67.64%]
     2,794,549,736 instructions              #    1.27  insns per cycle        
                                             #    0.31  stalled cycles per insn [84.75%]
       661,425,743 branches                  #  826.515 M/sec                   [84.04%]
         9,605,392 branch-misses             #    1.45% of all branches         [82.83%]

       0.739954252 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{ADEVS} - Classic DEVS}
\begin{Verbatim}[fontsize=\small]
$ perf stat ./sim 32 0

 Performance counter stats for './sim 32 0':

         15.465233 task-clock (msec)         #    0.976 CPUs utilized          
                 1 context-switches          #    0.065 K/sec                  
                 1 cpu-migrations            #    0.065 K/sec                  
               340 page-faults               #    0.022 M/sec                  
        22,395,954 cycles                    #    1.448 GHz                     [74.20%]
         6,618,757 stalled-cycles-frontend   #   29.55% frontend cycles idle    [74.17%]
         2,895,009 stalled-cycles-backend    #   12.93% backend  cycles idle    [87.88%]
        31,797,999 instructions              #    1.42  insns per cycle        
                                             #    0.21  stalled cycles per insn
         6,645,494 branches                  #  429.705 M/sec                  
           250,467 branch-misses             #    3.77% of all branches         [74.03%]

       0.015852985 seconds time elapsed
\end{Verbatim}

\section{Compared to PyPDEVS on PyPy}
Surprisingly, both DEVStone and PHOLD perform significantly worse using \textit{PyPy} instead of \textit{CPython}, despite the sometimes dramatically improved CPU utilization. Perhaps these two test cases just happen to be "JIT-unfriendly".\\
\textit{PyPy} is known to sometimes perform worse when profiling but informally its speed didn't seem any different when running the benchmarks without using \texttt{perf}.

\subsection{DEVStone}
\paragraph{\textsc{PythonPDEVS} - Classic DEVS on PyPy}
\begin{Verbatim}[fontsize=\small]
$ perf stat pypy experiment.py default 3 false > out.txt

 Performance counter stats for 'pypy experiment.py default 3 false':

       2367.165094 task-clock (msec)         #    0.998 CPUs utilized          
               276 context-switches          #    0.117 K/sec                  
                20 cpu-migrations            #    0.008 K/sec                  
            11,357 page-faults               #    0.005 M/sec                  
     6,765,226,427 cycles                    #    2.858 GHz                     [83.37%]
     4,234,478,457 stalled-cycles-frontend   #   62.59% frontend cycles idle    [83.27%]
     3,145,973,412 stalled-cycles-backend    #   46.50% backend  cycles idle    [66.67%]
     5,014,442,675 instructions              #    0.74  insns per cycle        
                                             #    0.84  stalled cycles per insn [83.49%]
     1,077,122,966 branches                  #  455.027 M/sec                   [83.44%]
        38,104,090 branch-misses             #    3.54% of all branches         [83.27%]

       2.372239526 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{PythonPDEVS} - Parallel DEVS on PyPy}
\begin{Verbatim}[fontsize=\small]
$ perf stat mpirun -np 2 pypy experiment.py default 3 false > out.txt

 Performance counter stats for 'mpirun -np 2 pypy experiment.py default 3 false':

       5789.446884 task-clock (msec)         #    1.991 CPUs utilized          
               624 context-switches          #    0.108 K/sec                  
                40 cpu-migrations            #    0.007 K/sec                  
            23,245 page-faults               #    0.004 M/sec                  
    15,328,712,962 cycles                    #    2.648 GHz                     [83.32%]
    10,258,365,662 stalled-cycles-frontend   #   66.92% frontend cycles idle    [83.32%]
     7,799,638,758 stalled-cycles-backend    #   50.88% backend  cycles idle    [66.54%]
    10,156,225,688 instructions              #    0.66  insns per cycle        
                                             #    1.01  stalled cycles per insn [83.23%]
     2,163,097,214 branches                  #  373.628 M/sec                   [83.47%]
        76,428,341 branch-misses             #    3.53% of all branches         [83.42%]

       2.907224325 seconds time elapsed
\end{Verbatim}
\subsection{PHOLD}
\paragraph{\textsc{PythonPDEVS} - Classic DEVS on PyPy}
\begin{Verbatim}[fontsize=\small]
$ perf stat pypy experiment.py > out.txt

 Performance counter stats for 'pypy experiment.py':

       1827.849910 task-clock (msec)         #    0.999 CPUs utilized          
               220 context-switches          #    0.120 K/sec                  
                10 cpu-migrations            #    0.005 K/sec                  
            14,385 page-faults               #    0.008 M/sec                  
     5,224,512,160 cycles                    #    2.858 GHz                     [83.37%]
     3,283,008,184 stalled-cycles-frontend   #   62.84% frontend cycles idle    [83.37%]
     2,563,366,886 stalled-cycles-backend    #   49.06% backend  cycles idle    [66.80%]
     4,079,915,131 instructions              #    0.78  insns per cycle        
                                             #    0.80  stalled cycles per insn [83.42%]
       758,611,592 branches                  #  415.029 M/sec                   [83.37%]
        28,841,978 branch-misses             #    3.80% of all branches         [83.16%]

       1.830573045 seconds time elapsed
\end{Verbatim}
\paragraph{\textsc{PythonPDEVS} - Parallel DEVS on PyPy}
\begin{Verbatim}[fontsize=\small]
$ perf stat mpirun -np 2 pypy experiment.py > out.txt

 Performance counter stats for 'mpirun -np 2 pypy experiment.py':

       5410.417506 task-clock (msec)         #    1.992 CPUs utilized          
               785 context-switches          #    0.145 K/sec                  
                33 cpu-migrations            #    0.006 K/sec                  
            28,874 page-faults               #    0.005 M/sec                  
    15,456,764,898 cycles                    #    2.857 GHz                     [83.35%]
    12,040,987,764 stalled-cycles-frontend   #   77.90% frontend cycles idle    [83.35%]
     6,550,209,612 stalled-cycles-backend    #   42.38% backend  cycles idle    [66.70%]
     7,709,537,731 instructions              #    0.50  insns per cycle        
                                             #    1.56  stalled cycles per insn [83.41%]
     1,430,492,329 branches                  #  264.396 M/sec                   [83.42%]
        63,358,010 branch-misses             #    4.43% of all branches         [83.21%]

       2.716329957 seconds time elapsed
\end{Verbatim}

\section{Logger}
We have also conducted a small benchmark to compare the raw power of our logger and the proposed g3log.
We let each logger print several hundred thousand messages to a file. These are the performance statistics:
\subsubsection{g3log}
\lstinputlisting[]{g3log.perf}
\subsubsection{Our own logger}
\lstinputlisting[]{own.perf}
\subsubsection{Conclusion}
While the difference is considerable, we should keep in mind that g3log generates timestamps for each message. It also contains several features that our logger doesn't, such as a more extensive crash safety. Moreover, g3log allows the user to dynamically change which log levels are filtered, while this can only be decided at compile time for our logger. This can be clearly seen in the amount of branches taken by the program.

\begin{thebibliography}{1}


  \bibitem{cow} \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=21334#c47}

  \bibitem{Mattern} \url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.2811}
  
  \bibitem{gil} \url{https://wiki.python.org/moin/GlobalInterpreterLock}
  
  \bibitem{tsan} \url{https://code.google.com/p/thread-sanitizer/}
  
  \bibitem{vg} \url{http://valgrind.org/}
  
  \bibitem{perf} \url{https://perf.wiki.kernel.org/index.php/Main_Page}
  
  \bibitem{cereal} \url{https://uscilab.github.io/cereal/}
  
  \bibitem{cpp} \url{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf}
  
  \bibitem{cons}  J. J. Nutaro, Building Software for Simulation: Theory and Algorithms, with
Applications in C++. Wiley Publishing, 2010.

  \bibitem{gpp}  \url{https://bugzilla.redhat.com/show_bug.cgi?id=1219175}
  
  \bibitem{gdb}  \url{https://bugzilla.redhat.com/show_bug.cgi?id=1219180}

  \bibitem{asynchwrite} \url{http://stackoverflow.com/a/21127776}

  \end{thebibliography}
\end{document}
