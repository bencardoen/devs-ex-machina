\documentclass[8pt,a4paper]{report}
\usepackage{hyperref}
\usepackage[latin1]{inputenc}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[scaled]{beramono}
\newcommand\Small{\fontsize{9}{9.2}\selectfont}
\newcommand*\LSTfont{\Small\ttfamily\SetTracking{encoding=*}{-60}\lsstyle}
\usepackage{url}
  \lstset{basicstyle=\ttfamily\tiny\color{blue}}
\usepackage{graphicx}
%\usepackage[]{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}

\title{Project Report\\\textsc{DEVS Ex Machina}}
\author{Ben Cardoen\\Pieter Lauwers\\Stijn Manhaeve\\Tim Tuijn\\Matthijs Van Os}
\date{}
\begin{document}
\maketitle
\tableofcontents{}

%
\chapter{Summary}
%
\section*{}
The project implements a C++ port of the PyPDEVS project, extended with conservative synchronization and a change in system architecture from distributed simulation on several nodes (asymmetric mp) to a symmetric multiprocessing architecture. \\The summarized goal of the project is to provide an implementation of the offered DEVS formalisms with significantly lower execution time by leveraging the smp architecture.

\section*{Terminology}
The literature sometimes exchanges terms, to avoid any confusion the following frequent terms are listed with their associated meaning as used in this document:
\begin{itemize}
  \item Core : alias kernel. Single instance, containing a set of models it is responsible for, exchanges messages generated by those models and keeps time state. Can correspond with Logical Process in terms of what it represents.
  \item Logical Process : set of models with strong internal coherence.
  \item Distributed parallelism : computation distributed over several physically separated machines communicating by means of a network stack.
  \item SMP : symmetric multiprocessing, running a parallel computation on several distinct CPU cores/threads on the same machine with shared memory (and/or cache).
  \item message : any exchange of high level information between entities, user defined or for inter-kernel synchronization.
  \item lookahead : Timespan relative to current state where the output of the model is not influenced by its input. (as per definition in \cite{cons}, app B)
  \item eot : Earliest output time. Timepoint in future where the core/kernel will output events.
  \item eit : Earliest input time. Timepoint in future where the core/kernel expects to
  be change state on receiving event.
\end{itemize}
Lookahead is collected per model per kernel, eit/eot are based on information from models but determined on a core/kernel level.

\chapter{Planning}
\section{Parallel}
\begin {itemize}
\item GVT integration : week 0-1, Ben and Tim
\item Conservative : week 2-3 , Ben and Tim
\end {itemize}
\section{General}
\begin {itemize}
\item Dynamic Structured : week 1-3 Stijn
\item DevStone and examples : week 0-2 Matthijs and Pieter
\item Serialization : week 0 Pieter
\item Simulation control : week 1-3 Matthijs and Pieter  
\item Tracers : week 0 Stijn
\item Benchmarking PyPDevs Adevs : week 3 Matthijs and Pieter
\end {itemize}
\section{Optimization}
Week 4 , general review.

\chapter{Architecture Analysis}
\section{Implementation language}
Python is a very powerful language, but due to its interpreted nature it suffers from a performance point of view. Python's GIL \cite{gil} makes this issue even more serious whenever multi-threading is required. Although it relieves the developer from tracking resource allocation, this comes at a price in the overhead caused by the GC. \\
In contrast, C++ offers low-level powerful thread primitives, but requires the developer to have a very deep understanding of parallel programming techniques. By using RAII, a C++ developer can leverage safe resource (de)allocation without losing performance.\\
A significant advantage Python offers a developer is relative cross-platform capability. This is non-trivial for a C++ developer, since at the very least the toolchains will be radically different in usage, and sometimes even in implementation of the C++ standard. The edit-build-test cycle is also non-trivial for a large C++ application, whereas in Python this is negligible. In contrast, Python does not offer the programmer any of the C++ compiler's many compile time checks (static typechecks and program analysis).

\section{Operating system support}
The project requires a compliant C++11 compiler and library, but beyond that is not really bound to special features. For testing, GTest is a requirement, and the only two other dependencies are the ubiquitous Boost and Cereal, the last is provided with the project as it is header only.

\section{Object representation}
In Python everything is a reference, and since the DEVS formalisms require that the user be allowed to override most of the basic entities, the best representation for an object was a shared\textunderscore ptr. Passing by value could have been safer (e.g. between threads) but would introduce slicing.\\ Another advantage was that we would use as little memory as possible, for example sending a message means passing an const reference to a shared pointer, which is usually equal in size to 2 pointers. By passing by const ref we minimize most of the penalty usually involved in copyconstructing shared pointers. \\
The only downside is the synchronized access to the reference count in the shared pointer. There is also a need for more boilerplate code to use shared pointers in containers (providing pass-through hash function, comparison etc). 

\section{Messaging Size}
A message is constructed once, and after that never copied. Messages are sent and received by reference to smart pointer, incurring a size cost of (on 64-bit platforms) 128bit. The payload (and its size) is irrelevant to the simulation up to the point where the message is actually received. A default implementation allows the payload stored as a std::string, but a specialized message class can be used for sending objects of a different type. When constructing a message by using the functionality provided by the Port class, this happens automatically. When receiving messages, a free function template is provided to extract the object from the message.\\
For example:
\begin{lstlisting}[language=C++,basicstyle=\small\color{black}]
//initialization of necessary objects is left out.
double value;
n_model::t_portptr port;

//create messages that contain a double
auto msgVector = port->createMessages(value);

//say we receive this message somewhere else
n_network::t_msgptr received =  msgVector[0];

//retrieve the data when receiving the message
const double& data = n_network::getMsgPayload<double>(received);

//Assuming that no Z function was applied to the message.
assert(value == data);
\end{lstlisting}
\paragraph{Note} While it is technically possible, we strongly discourage the usage of the following data types in these messages:
\begin{itemize}
	\item raw pointers to heap-allocated object.\\
			Messages can be destroyed during a time warp prior to being delivered to a model. This will result in a memory leak if there are no other references to these objects.
	\item pointers or references to local objects.\\
		When these objects go out of scope, they are destructed, rendering the pointer or reference invalid.
\end{itemize}

\chapter{Design}
\section{Overview}
We've slightly departed from the PyPDEVS structure, to allow for more encapsulation and decoupling of the core entities. This clear separation of data and responsibilities between classes made the threading part of the project easier (no or minimal shared state). See the \hyperref[simpledesign]{Simple Design figure \ref*{simpledesign}}.\\

\begin{figure}[h!]
	\makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{Design.png}}
	\caption{Simple Design}
	\label{simpledesign}
\end{figure}

\section{Translating Dynamic Typing to Static Typing}
Certain classes of the simulation, such as the messages between models and the model states, must be able to contain objects of any type. However, C++ is a statically typed language. To overcome this problem, we came up with the following three solutions:
\begin{itemize}
	\item Keep a \texttt{void*} that points to the actual data. This approach is very fragile and impossible to serialize. Misuse of \texttt{void*} could result in anything from undefined behaviour to stack corruption.
	\item Serialize everything to \texttt{std::string}. Serialization is not a trivial operation. Continuously serializing and deserializing can be a very large performance hit. Moreover, it would require each and every type to be serializable. On the flip side, the compiler can help by detecting whether or not the object can be serialized or not.
	\item Make use of inheritance and create a specialized subclass for each and every datatype. In the case of the messages, a class template can be used to facilitate this process. In the case of the model states, we decided that it is best that the user provides the subclass.\\
	Since we use pointers for messages and states, a pointer cast is all that is needed to access the members of the specialized class. For extra safety, we suggest to use \texttt{std::dynamic\_pointer\_cast}.
\end{itemize}
We chose the third option because it both offers speed and safety.

\section{Class decomposition}
The active entities in the project were discovered using several methods: use cases, argumentation and counterargumentation during the weekly meetings, and studying the existing implementation. This led to the following simplified decomposition:
\begin{enumerate}
\item Model : provides DEVS formalism implementation of an Coupled/Atomic model.
\item Core : drives transitions, exchanges messages, synchronizes
\item Controller : Constructs simulator, controls simulation
\item Tracer : Traces output
\end{enumerate}
The other classes bind/create instances of the above (e.g. LocationTable, Allocator, Factory), or provide essential functionality (e.g. Message, Network, Scheduler)\\
For a full overview, please consult the documentation generated by doxygen.

\section{Network -- Kernel communication}
PyPDEVS uses MPI to exchange messages between kernels, whereas we could use shared memory to communicate between kernels (each running on 1 or more threads). We created a network architecture mimicking the approach used by PyPDEVS, without sacrificing the speed and ordering advantages of shared memory. The Network class consists of a single locked receiving port, and has for each existing kernel an outgoing queue. Receiving a message locks the network for the time it needs to place it on an integer indexed queue (very fast). If a kernel wants to receive messages it can do so if/when it is ready to do so, this only locks if at the same time a message is being received by the network.\\ Several kernels can pull messages without contention in parallel from the network (since the output queues are not shared). \\This is illustrated in \hyperref[networkdesign]{Network design figure \ref*{networkdesign}}.

\begin{figure}[h!]
	\makebox[\textwidth][c]{\includegraphics[width=\textwidth]{network.png}}
	\caption{Network cooperation diagram}
	\label{networkdesign}
\end{figure} 
Early benchmarks showed a repeatable throughput of 5e5 msg/s, with 4 kernels on modest hardware. The problems of (unintentional) out of order receipt and potential loss of messages are a non-issue in this context, since we use FIFO queues and loss of a message would implicate severe system failure (in which case message loss is the lesser problem).\\
Another advantage of hiding the message exchange behind a shared Network class instance is that, should this be required in the future, the kernel could run on different machines without requiring much redesigning of the system. An RPC implementation would simply have to implement the Network class on both sides. \\
Late in the project functionality was added to check if the network contained any transient messages. This is not required for any algorithms, but to allow Cores to safely stop in an optimistic simulation. Due to OS-scheduling of threads it is entirely possible for all cores to report being idle (having reached termination time) and with a message still in the network. The controller would decide the simulation is done and exit, which would be incorrect. Synchronizing the steps which decide if a core is idle and if there are messages would be more expensive than adding this functionality with an atomic boolean to the network.

\section{Tracing}
The tracing uses template programming, and is largely asynchronous w.r.t. the simulator kernels. This can be seen by the high utilization values when benchmarking the examples.
The tracing component makes heavy use of template programming. For more detailed information, please refer to \href{run:./tracersReport.pdf}{tracersReport.pdf}.

\section{Models}
The models hide all implementation details for the particular DEVS formalisms, without limiting the user to extend into any subclassed model of his/her choice. The use can focus on what he/she wants the model to emulate, and not worry about for example locking, allocation etc. We refer to the manual for more information about the creation of these models.

\section{Scheduler}
A Scheduler was implemented based on the prototyping code. As required, as list based scheduler was implemented. \\
The interface from PyPDevs' scheduler class was reused, but internally the scheduler class in our project differs on a few points. \\
\subsection{Composition}
All schedulers are based on a heap, which order items to be scheduled. We never actually schedule models, only entries (A struct containing name and time to schedule), allowing for far faster operation. This is complemented by a hashmap, so that entries can have O(1) lookup and erasure (required for the use case of antimessaging, among others). The operation massReschedule() was left out of the interface, since it can be easily composed using erase() and push\textunderscore back().
\\
\subsection{Reuse by other classes.}
Several classes in the project require a heap-like functionality, by making the Scheduler class as generic as possible, we can reuse the same code instead of limiting its use to scheduling models only.
The Scheduler class is  reused by the Tracers (scheduling tracing events) and the messaging queue in a kernel. Each client class can select at construction which heap backend to use, and if single operation synchronization is desired.
\subsection{Entries}
A scheduler instance can schedule any item provided it has a well defined implementation of the hash function and operator less than. In practice however, we used specialized Entry structs, so that intrusion upon the scheduled items was not necessary. \\
Consider for example scheduling a model: \\ If the scheduling code relies only on the hash function of the model, this implementation can never change. But if we use a struct holding a reference/pointer/name to the model as the scheduled item, we can redefine any operator without interfering with other client classes of Model. This for example occurs in the Scheduler that holds pending incoming messages in the Kernel/Core, and the Tracers which has a scheduler for a subclass of Message. Both use specific locally defined entry classes without influencing each other.
\subsection{Implementation}
The actual heap used is either based on std::list as required, or one of Boost's heaps. The user is provided with a convenience factory function that creates Scheduler instances with varying heap backends. In general, the Fibonacci heap is one of the fastest, with skew heap sometimes coming out ahead. The list based heap is usually slower, though again this depends on the usage scenario. \\Important to note is that only node based heaps can be used efficiently (without compromising the performance of erase() and contains()). We store on insertion an iterator with the key of the pushed element in the hashmap. This also works as a safety check, preventing double insertion (depending on the hash-specialization semantics). This freedom of implementation is visible in the tracers where duplicate elements are allowed if they have a different address, contrasted with the model scheduler which disallows a model to be scheduled twice.\\
Finally, the std::hash$<>$ specialization used by the message scheduler allows O(1) annihilation of a message by its antimessage (since the hash is identical). Similar usage of erase() is required in the model scheduler to quickly invalidate old scheduled entries (massReschedule).

\subsection{ADevs scheduler}
The adevs scheduler is a binary heap, but does not allow erasure of entries which would render it incapable of fulfilling the Pdevs scheduler interface. The binary heap (e.g. stl/boost priority queue or any container + heap operations in stl) can't be used since we need to store pointers to entries, which requires a node based heap. \\It can be emulated by using the faster Fibonacci or skew heaps, or the slower list based heap. \\
Note that Adevs's implementation is intrusive upon the model  (q\textunderscore index), which could make the scheduling code vulnerable. \\
Should we implement a binary heap scheduler without nodes, the pointer correcting code on any heap operation would prohibit scaling, since any heap operation would invalidate the iterators pointing to the entries. Furthermore, a whole series of operations on non node based containers (eg vector) have this same invalidating behaviour, making the implementation unnecessarily complex.

\section{Controller}
For the controller, care was taken in making the interface both functional and minimal. The default configuration is quick to set up, and customization is possible should the user require it.

\section{Serialization}
For serialization we used the Cereal \cite{cereal} header-only framework. We have used separate load and save functions for full flexibility. For each type that occurs in the form of a pointer we implemented a static load and construct function since the object first has to be created before the saved data can be imported. Cereal has a small coupling with the code (models) that has to be written by the user as the user has to register his own classes due to polymorphism.

\chapter{Non-functional requirements}
\section{Correctness}
\subsubsection{Testing}
The use of the jenkins CI was heavily leveraged, in combination with extensive testing. Any feature increment could only be merged into the project if it passed the developer's own jenkins instance.
\subsubsection{Thread races and deadlock}
Deadlocks and race conditions are very hard to debug, let alone prevent. To help us in this area as much as possible we used prototypes of threaded code in the tests before merging them into the main classes, and each build is checked with ThreadSanitizer \cite{tsan} which proved invaluable in preventing races and deadlocks in the threaded codepaths.
\subsubsection{Memory leaks and undefined behaviour}
With most entities represented as smart pointer, the need to ensure a leak free simulator nevertheless remained. To check for allocation misuse and a whole other series of errors we used Valgrind \cite{vg}. This meant for example that we detected a leak caused by circular referencing smart pointers.

\subsubsection{Teamwork}
We met at least once every week (barring the exam period), with each meeting having a formal report. Whenever required, teammembers used video conferencing to solve critical bugs or discuss design/coding issues.
\subsubsection{Code review}
Although there was no real formal code review done up to now, we've had several debug sessions reviewing each others code. This means the most critical used sections of the code are allready reasonably well reviewed, albeit not formally.
\subsubsection{Portability}
We develop on Unix, Windows and MacOSX.

In Windows we chose the Cygwin platform instead of MinGW to compile our project on. This because we make use of the C++11 std::thread objects; MinGW doesn't support POSIX threads. When building the project with GTest on Windows you need to use the -std=gnu++11 flag instead of the standard -std=c++11 flag, because we need the GNU extensions to correctly link everything.

\chapter{Functional requirements}
\section{DEVS Formalisms}
\subsection{Single core}
The single core implementations follow exactly the runtime behaviour shown by the PyPDEVS simulator. We've chosen to emulate the Select() function with an integer priority value the user can set. This saves 1 virtual function call per transition and can quite easily be optimized by the compiler since it remains constant.
\subsubsection{DirectConnect}
The directConnect algorithm has gone relatively unchanged from its implementation in PyPDEVS, though the role of the RootDEVS class has been simplified somewhat. Its main function has been reduced to executing directConnect and then storing the resulting list of connected atomic models.
\subsubsection{Dynamic Structured}
Dynamic structured DEVS is completely supported in single core simulation. This includes allowing the following structural changes during the simulation:
\begin{itemize}
	\item adding/removing connections
	\item adding/removing ports
	\item adding/removing submodels.
\end{itemize}
These changes are only allowed during a special phase of the simulation. The simulator provides a map object of type \lstinline[language=C++,basicstyle=\small\color{black}]{std::multimap<std::string, std::string>} that can be used for communication and cooperation between the models. During one simulation step, this communication always happens from bottom to top. Only models that had an internal transition are considered for performing dynamic structured.\\
Unlike the PyPDEVS, we will try to keep a removed model intact when it is removed from the simulation.
\subsection{Parallel}
\subsubsection{Optimistic}
Optimistic time synchronization requires an implementation of timewarp, and more extensive locking than would be required for conservative. The GVT algorithm (Mattern \cite{Mattern}) runs on a separate thread from the simulation kernel threads, so care must be taken that they don't deadlock, but don't race either.
\subsubsection{Conservative}
The implementation of the conservative formalism follows the algorithm as described in \cite{cons}. \\
\paragraph{Algorithm}
The following listing briefly states the steps required in the conservative synchronization algorithm.\\
Initialize eit,eot as (0,0) per kernel.\\
A Time object has an absolute first field , followed by a causal second field.\\
\begin{algorithm}
\caption{Conservative PDEVS}
\label{CHalgorithm}
\begin{algorithmic}
\Procedure{CPDEVS Modified}{}
\State 0.1 Construct a depency graph, determining which models influence this kernel directly
\State 0.2 If a msg is sent : eot = (timestamp > eot) ? timestamp : eot
\While{Termination trigger not reached}
\State 1. Simulate until vtime $>=$ eit
\State 1.a If msg is sent during sim : eot = max(timestamp, eot)
\State 2.a Time x = (eit + min lookahead, 0)
\State 2.b Time y = (msg sent) ? (eit,1) : (time next transition)
\State 2.c Time eot = std::min(x,y)
\State 2.d Share eot value with other kernels.
\State 3   Time eit = min ( $eot_i$ for i in influencing kernels)
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}\\
\subparagraph{Differences with ADEVS}
A big difference between ADEVS' and our implementation is that we hold the eot-values of all kernels in shared memory. This means that step 1.a , which is normally done by the receiver when it inspects the message queue, can be done immediately. This also avoids the queueing delay incurred by simulation traffic, and conversely does not interfere with the simulation traffic.\\
Using the shared memory approach, we also avoid the sending of any synchronization message to other cores (required in ADEVS). \\In step 2.d we simply overwrite our old EOT value with the newly calculated. The shared EOT vector is only written at 1 position for each kernel , and only the influencing EOT values are ever read. \\
Also important to note is that there exist several edge cases not listed in this algorithm, but which force the implementation to do more work than listed above. Consider a kernel with a non-dependent model A (that receives no input). By definition A has eit = oo, causing all eot-calculation to be skipped forever. We can't do this, if there exists at least 1 model depending on A, this model would get incorrect eot-values (either 0 or oo, which neither are correct since A is not prohibited from sending output).\\
The correct behaviour here is to replace eit with the vtime of the kernel at each simulation step and to use that in the algorithm.\\
 Furthermore, collecting lookahead values from models needs to be done in step 1.a. The lookahead value is calculated starting from the time it acquires a new state, this means that the lookahead value used in 2.a has to be continuously updated, regardless of usage, and has to be reset between simulation rounds (step 1 can entail several of such rounds in the theoretic form of the algorithm). If we don't reset the minimum lookahead value in between rounds, there are scenarios where an incorrect minimum of a model that is no longer scheduled in the next round can warp the calculation, potentially freezing the simulation. \\ 
 The calculation done in steps 2, 3 are quite efficiently implemented, although a future optimization could delay step 3 until several rounds have passed, more in line with the theoretical algorithm. For details please consult the source code of Doxygen documentation of class Conservativecore.
\subparagraph{Dependency calculation}
The author of the models needs to supply the kernel, for each model, with a list of models by whom it is influenced. Receiving a message that alters a model's output is defined as influencing in this context. Using the allocation information provided by the Controller, the Kernel will then determine which other Kernels (including possibly itself) it is dependent on.\\
It is important to note that the user need not interpret this dependency relation as transitive, and needs to look only at 1-edge removed models. \\
To clarify this last point: suppose we have 3 models A, B, C with A sending output to B, and B to C. C is indirectly influenced by A, but need not record this information (which is non trivial to retrieve for a non-trivial model, disregarding cycles for the moment). \\
Let A be allocated @ kernel 0, B @ 1, C @ 2. While the dependency is transitive, since kernel 1 is already bound by kernel 0, kernel 2 does not require kernel 0's information (eot). By moving this transitivity to the kernel level, the model's author only has to look at 1-edge distant dependencies. \\
At construction a Conservative Kernel will collect , for each model it holds, the list of names of influencing models. Next, the kernel requests for each unique name the allocation information (O(1) lookup). Note that this can be optimized if the model stores its allocation information, but this then prohibits runtime reallocation should this be implemented in the future.

\chapter{Obstacles}
\section{Virtualization}
Virtualization proved a significant hurdle by exposing potential faults. Standard compliant code was found to be leaking memory by not calling container member destructors but only on a VM with Linux as Guest and Windows as Host. This was never reproducible in another setting.
\section{Threading}
\subsection{String}
libstdc++'s std::string implementation appears to be still Copy On Write (despite 21.4.1 p6 from the C++ standard) \cite{cpp}. This triggers an alert by threadsanitizer for a possible race whenever a string was supposedly copied but internally still stored as a reference to the same object. Explicitly invoking the string constructor with the char[] of the original string, proved to be a safe workaround here. Note that this problem is a known issue \cite{cow}, which is solved in GCC 5.1.\\
\subsection{Timing}
We also encountered several scenarios where thread scheduling complicated our kernel control code. An extreme example of this is Valgrind, which can cause multithreaded code to be executed sequentially. This can be fatal for a parallel simulator if a dependent core is scheduled before the influencing core is ever executed. Modifying valgrind's invocation can alleviate this, but there remain edge cases where a core/kernel can get into a state where it can no longer advance (depending on the model composition). The simulator detects this and (eventually) halts the simulation. \\ Note that the above problem is not limited to valgrind, on a virtualized OS this is more easily triggered than in a non virtualized setting.\\
Furthermore, we also force an idle kernel (a kernel that has either reached termination time, or can't advance time (no messages and no scheduled models) to yield/sleep for a short duration. The reason for this behaviour is to try to grant the other cores execution time, possibly alleviating thread starvation where an idle core (but non-idle thread) would starve a non-idle core. A condition variable was evaluated as a solution, but not implemented since it also suffered from timing issues (time between thread awakening and core simulating), and would conflict with the controlling code (save,pause) already in place. 
\section{Heaps}
2 of the Heaps we use in the Scheduler proved to have subtle corrupting bugs, which were only found quite late in the project. Boost's binomial heap, for reasons so far unknown, sometimes fails to deallocate/remove nodes/items. The actual failure is masked until the next insertion, where we then triggered one of our own invariant checks (the heap size was no longer equal to that of the hashmap). It should be noted that we already experienced similar issues with the binomial heap not deallocating memory despite size() returning 0. \\ Time prevented us from writing up a formal bug report, since the bug is hard to reproduce and even harder to detect.
The D-ary heap triggered a "move-assign to self" assertion when we used the Debugging version of the STL, so we had to remove both from our Schedulers. This is no way impacted performance, as the Fibonacci heap was in practice faster (with the skew heap sometimes overtaking it).
\section{Toolchains}
To develop we used the required toolchains:\\ g++/clang++ as compiler, Eclipse+CDT as IDE, git for dvcs, jenkins as ci, cmake as build tool supplemented with some shell and python scripting.
The plot plugin from Jenkins was used to do perfomance regression testing, but the plugin itself is not that stable. Nonetheless, we detected several random regression using the plotted graphs. \\
G++ failed with an internal compiler error in certain multi-build cmake scenario's, for which a  bug was filed \cite{gpp}. GDB also intermittently failed with a segmentation fault when debugging threaded code. Using Fedora's bug reporting mechanism abrt a bug \cite{gdb} was filed for this as well.
\section{Logger}
The recommended logger (g3log), while powerful and fast, proved a dead end since it does not compile on Cygwin. 
Stijn therefore wrote his own implementation.\\
This implementation only covers the most basic needs:
\begin{itemize}
	\item asynchronous output
	\item thread safe
	\item multiple logging levels currently, the following 4 levels are supported:\\
		\emph{INFO}, \emph{DEBUG}, \emph{WARNING} and \emph{ERROR}
	\item levels can be individually switched on/off at compile time
	\item No logging code is executed whatsoever if the logger is disabled.
	\item log messages contain file name and line number of the log command.
	\item basic crash safety. The logger will catch the following signals and flush all remaining output before exiting the program:
		\begin{itemize}
			\item \emph{SIGSEGV} Segmentation fault, sent by the operating system.
			\item \emph{SIGTERM} Termination request, sent by the program.
			\item \emph{SIGABRT} Abort --- abnormal termination, e.g. when an assertion fails.
			\item \emph{SIGINT} Interrupt, used for catching crtl+C, which is useful for debugging deadlocks.
		\end{itemize}
\end{itemize}

\paragraph{Asynchronous output}
To implement the asynchronous output, we made use of a custom stream buffer. The original design is by Dietmar K{\"u}hl \cite{asynchwrite}. We modified this design in order to make it thread safe.

\chapter{Performance Analysis}
Whenever we made big design/architecture decisions, we first wrote some prototyping testcode to prove the viability (e.g. Network, Tracing, Logger).
\section{First comparison}
We use perf to get statistically sound benchmarks. The command run was :
\lstinline!$perf stat -r {rounds} {command}!\\

\subsection{Classic}
An endtime of 3.6e6 was set, and both examples run against each other.
\lstinputlisting[]{pycla.txt}
The following shows a recording of the C++ implementation of the same example.\\
\lstinputlisting[]{cpcla.txt}
\subsubsection{Conclusion}
The C++ port is approximately 12.906 times faster.

\subsection{Dynamic structured}
An end-time of 3.6e6 was set, and both examples run against each other.
\lstinputlisting[]{pydyn.txt}
The following shows a recording of the C++ implementation of the same example.\\
\lstinputlisting[]{cpdyn.txt}
\subsubsection{Conclusion}
This result is still warped due to the startup/shutdown time of GTEST, but allready the C++ port is roughly 7.05 times faster, only page faults and migrations are worse (but being worked on).
\subsection{Parallel}
With GVT not yet active, an endtime of 3.6e6 was used (as before).\lstinputlisting[]{pypar.txt}\
The following shows a recording of the C++ implementation of the same example.\\
\lstinputlisting[]{cppar.txt}
\subsubsection{Conclusion}
The C++ port is approximately 19.6 times faster.

\section{Logger}
We have also conducted a small benchmark to compare the raw power of our logger and the proposed g3log.
We let each logger print several hundred thousand messages to a file. These are the performance statistics:
\subsubsection{g3log}
\lstinputlisting[]{g3log.perf}
\subsubsection{Our own logger}
\lstinputlisting[]{own.perf}
\subsubsection{Conclusion}
While the difference is considerable, we should keep in mind that g3log generates timestamps for each message. It also contains several features that our logger doesn't, such as a more extensive crash safety. Moreover, g3log allows the user to dynamically change which log levels are filtered, while this can only be decided at compile time for our logger. This can be clearly seen in the amount of branches taken by the program.

\begin{thebibliography}{1}


  \bibitem{cow} \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=21334#c47}

  \bibitem{Mattern} \url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.2811}
  
  \bibitem{gil} \url{https://wiki.python.org/moin/GlobalInterpreterLock}
  
  \bibitem{tsan} \url{https://code.google.com/p/thread-sanitizer/}
  
  \bibitem{vg} \url{http://valgrind.org/}
  
  \bibitem{perf} \url{https://perf.wiki.kernel.org/index.php/Main_Page}
  
  \bibitem{cereal} \url{https://uscilab.github.io/cereal/}
  
  \bibitem{cpp} \url{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf}
  
  \bibitem{cons}  J. J. Nutaro, Building Software for Simulation: Theory and Algorithms, with
Applications in C++. Wiley Publishing, 2010.

  \bibitem{gpp}  \url{https://bugzilla.redhat.com/show_bug.cgi?id=1219175}
  
  \bibitem{gdb}  \url{https://bugzilla.redhat.com/show_bug.cgi?id=1219180}

  \bibitem{asynchwrite} \url{http://stackoverflow.com/a/21127776}

  \end{thebibliography}
\end{document}
