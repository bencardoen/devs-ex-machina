\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{amsfonts}
\usepackage{amssymb}

\date{}
\title{\textsc{DEVS Ex Machina} \\ Performance Report}
\begin{document}
\maketitle

\chapter*{DEVStone}
To measure the performance of \textsc{DEVS Ex Machina} under more demanding conditions, \textbf{DEVStone} and \textbf{PHOLD} benchmarks were used. Everytime we compare the performance of our implementation with \textit{PythonPDEVS}.\\

\section{Classic DEVS}
Using Classic DEVS we see noticeable speedups compared to PyPDEVS.
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_devstone 

 Performance counter stats for './dxexmachina_devstone':

        314.286326 task-clock (msec)         #    1.125 CPUs utilized          
               410 context-switches          #    0.001 M/sec                  
             1,007 cpu-migrations            #    0.003 M/sec                  
               662 page-faults               #    0.002 M/sec                  
       802,295,514 cycles                    #    2.553 GHz                     [85.27%]
       447,423,794 stalled-cycles-frontend   #   55.77% frontend cycles idle    [84.45%]
       234,246,612 stalled-cycles-backend    #   29.20% backend  cycles idle    [71.04%]
       739,746,191 instructions              #    0.92  insns per cycle        
                                             #    0.60  stalled cycles per insn [85.97%]
       161,577,264 branches                  #  514.108 M/sec                   [85.84%]
         3,030,149 branch-misses             #    1.88% of all branches         [83.25%]

       0.279274362 seconds time elapsed
\end{Verbatim}

\begin{Verbatim}[fontsize=\small]
$ perf stat python experiment.py default 2 false > out.txt

 Performance counter stats for 'python experiment.py def 2 false':

        709.776558 task-clock (msec)         #    0.989 CPUs utilized          
               191 context-switches          #    0.269 K/sec                  
                34 cpu-migrations            #    0.048 K/sec                  
             2,942 page-faults               #    0.004 M/sec                  
     1,687,298,167 cycles                    #    2.377 GHz                     [82.71%]
       866,759,527 stalled-cycles-frontend   #   51.37% frontend cycles idle    [83.25%]
       562,247,464 stalled-cycles-backend    #   33.32% backend  cycles idle    [67.62%]
     1,672,919,036 instructions              #    0.99  insns per cycle        
                                             #    0.52  stalled cycles per insn [83.82%]
       375,443,818 branches                  #  528.961 M/sec                   [83.25%]
        14,867,814 branch-misses             #    3.96% of all branches         [83.28%]

       0.717576473 seconds time elapsed
\end{Verbatim}

\section{Parallel DEVS}
Unfortunately, there's quite a big gap in performance between PyPDEVS and DEVS Ex Machina in parallel simulation. The issues causing this are being ironed out.
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_devstone 

 Performance counter stats for './dxexmachina_devstone':

       2666.499943 task-clock (msec)         #    0.017 CPUs utilized          
            16,835 context-switches          #    0.006 M/sec                  
               376 cpu-migrations            #    0.141 K/sec                  
             1,865 page-faults               #    0.699 K/sec                  
     5,780,070,450 cycles                    #    2.168 GHz                     [86.46%]
     2,313,049,054 stalled-cycles-frontend   #   40.02% frontend cycles idle    [84.79%]
     1,523,702,578 stalled-cycles-backend    #   26.36% backend  cycles idle    [58.20%]
     6,862,832,656 instructions              #    1.19  insns per cycle        
                                             #    0.34  stalled cycles per insn [83.73%]
     1,698,388,723 branches                  #  636.936 M/sec                   [85.64%]
        25,395,874 branch-misses             #    1.50% of all branches         [85.64%]

     156.945218942 seconds time elapsed
\end{Verbatim}

\begin{Verbatim}[fontsize=\small]
$ perf stat mpirun -np 2 python experiment.py def 2 false > out.txt

 Performance counter stats for 'mpirun -np 2 python experiment.py def 2 false':

        787.125992 task-clock (msec)         #    0.371 CPUs utilized          
             1,120 context-switches          #    0.001 M/sec                  
                91 cpu-migrations            #    0.116 K/sec                  
             8,684 page-faults               #    0.011 M/sec                  
     1,786,217,349 cycles                    #    2.269 GHz                     [83.85%]
       935,480,076 stalled-cycles-frontend   #   52.37% frontend cycles idle    [84.18%]
       619,366,137 stalled-cycles-backend    #   34.67% backend  cycles idle    [65.26%]
     1,796,463,414 instructions              #    1.01  insns per cycle        
                                             #    0.52  stalled cycles per insn [84.16%]
       403,042,819 branches                  #  512.044 M/sec                   [84.70%]
        13,210,328 branch-misses             #    3.28% of all branches         [82.68%]

       2.123888919 seconds time elapsed
\end{Verbatim}

\chapter*{PHOLD}
\section{Classic DEVS}
Just like DEVStone, PHOLD is quite a bit faster using our implementation. The CPU is used more efficiently.
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_phold 

 Performance counter stats for './dxexmachina_phold':

         24.249048 task-clock (msec)         #    1.109 CPUs utilized          
                16 context-switches          #    0.660 K/sec                  
               117 cpu-migrations            #    0.005 M/sec                  
               688 page-faults               #    0.028 M/sec                  
        52,853,150 cycles                    #    2.180 GHz                     [83.16%]
        29,014,020 stalled-cycles-frontend   #   54.90% frontend cycles idle    [83.16%]
        14,103,460 stalled-cycles-backend    #   26.68% backend  cycles idle    [65.29%]
        38,553,838 instructions              #    0.73  insns per cycle        
                                             #    0.75  stalled cycles per insn [83.31%]
         8,242,520 branches                  #  339.911 M/sec                   [96.13%]
           185,357 branch-misses             #    2.25% of all branches         [83.71%]

       0.021871681 seconds time elapsed
\end{Verbatim}

\begin{Verbatim}[fontsize=\small]
$ perf stat python experiment.py def 2 false > out.txt

 Performance counter stats for 'python experiment.py def 2 false':

        709.776558 task-clock (msec)         #    0.989 CPUs utilized          
               191 context-switches          #    0.269 K/sec                  
                34 cpu-migrations            #    0.048 K/sec                  
             2,942 page-faults               #    0.004 M/sec                  
     1,687,298,167 cycles                    #    2.377 GHz                     [82.71%]
       866,759,527 stalled-cycles-frontend   #   51.37% frontend cycles idle    [83.25%]
       562,247,464 stalled-cycles-backend    #   33.32% backend  cycles idle    [67.62%]
     1,672,919,036 instructions              #    0.99  insns per cycle        
                                             #    0.52  stalled cycles per insn [83.82%]
       375,443,818 branches                  #  528.961 M/sec                   [83.25%]
        14,867,814 branch-misses             #    3.96% of all branches         [83.28%]

       0.717576473 seconds time elapsed
\end{Verbatim}

\section{Parallel DEVS}
PHOLD in parallel on PyPDEVS crashed immediately with an MPI error, hence a comparison was not possible. However, given the results of our implementation we can assume that here too, the parallel simulation would be quite a bit slower.
\begin{Verbatim}[fontsize=\small]
$ perf stat ./dxexmachina_phold 

 Performance counter stats for './dxexmachina_phold':

        820.288154 task-clock (msec)         #    0.003 CPUs utilized          
            20,030 context-switches          #    0.024 M/sec                  
               331 cpu-migrations            #    0.404 K/sec                  
               831 page-faults               #    0.001 M/sec                  
     1,025,470,316 cycles                    #    1.250 GHz                     [97.69%]
       767,298,149 stalled-cycles-frontend   #   74.82% frontend cycles idle    [99.29%]
       536,886,871 stalled-cycles-backend    #   52.36% backend  cycles idle    [52.05%]
       480,746,733 instructions              #    0.47  insns per cycle        
                                             #    1.60  stalled cycles per insn [99.54%]
       118,012,980 branches                  #  143.868 M/sec                   [53.18%]
         8,481,176 branch-misses             #    7.19% of all branches         [99.62%]

     247.566439823 seconds time elapsed
\end{Verbatim}

\end{document}