\textsf{DEVS}~\cite{ClassicDEVS} is a popular formalism for modelling complex dynamic systems using a discrete-event abstraction.
In fact, it can serve as a simulation ``assembly language'' to which models in other formalisms can be mapped~\cite{DEVSbase}.
A number of tools have been constructed by academia and industry that allow the modelling and simulation of \textsf{DEVS} models.

But with the ever increasing complexity of simulation models, parallel simulation becomes necessary to perform the simulation within reasonable time bounds.
Whereas \textsf{Parallel DEVS}~\cite{ParallelDEVS} was introduced to increase parallelism, its inherent parallelism is often insufficient~\cite{Himmelspach}.
Several synchronization protocols from the discrete event simulation community~\cite{FujimotoBook} have been applied to (\textsf{Parallel}) \textsf{DEVS} simulation~\cite{globaltimewarp}.
With synchronization protocols, different simulation kernels can be at different points in simulated time, significantly increasing parallelism at the cost of synchronization overhead.
While several parallel \textsf{DEVS} simulation tools exist, they are often limited to a single synchronization protocol.
The reason for different synchronization protocols, however, is that their distinct nature makes them applicable in different situations, each outperforming the other for specific models~\cite{Jafer}.
The parallel simulation capabilities of current tools are therefore limited to specific domains.

This paper introduces \textit{DEVS-Ex-Machina}\footnote{\url{https://bitbucket.org/bcardoen/devs-ex-machina}} (``\textit{dxex}''): our simulation tool~\cite{dxex} which offers 6 synchronization protocols.
The simulation tool can spawn multiple kernels, each of which has its own simulation time and simulation control, though they may exchange timestamped events with each other.
Each kernel can spawn multiple threads, each of which will be mapped to a physical CPU core by the operating system.
We note the presence of two kinds of synchronization: inter-kernel and intra-kernel.
Inter-kernel synchronization uses either no synchronization, conservative synchronization, or optimistic synchronization.
Intra-kernel synchronization uses either no synchronization or exploits the inherent parallelism of Parallel DEVS to allocate concurrent transitions to separate threads.
The inherent parallelism in the intra-kernel synchronization protocol is used \pSim in the remainder of this paper.

This results in the following set of synchronization protocol combinations:
\begin{itemize}
\item No inter-kernel synchronization, no intra-kernel synchronization (\textit{NN})
\item Conservative inter-kernel synchronization, no intra-kernel synchronization (\textit{CN})
\item Optimistic inter-kernel synchronization, no intra-kernel synchronization (\textit{ON})
\item No inter-kernel synchronization, \pSim intra-kernel synchronization (\textit{NP})
\item Conservative inter-kernel synchronization, \pSim intra-kernel synchronization (\textit{CP})
\item Optimistic inter-kernel synchronization, \pSim intra-kernels synchronization (\textit{OP})
\end{itemize}

When refering to a set of them, we use the notation \textit{x} to denote any possible algorithm.
For example, \textit{xP} refers to NP, CP, and OP.

The selected synchronization protocol is transparent to the simulated model: users only determine the protocol to use.
Users who simulate a wide variety of models, with different ideal synchronization protocols, can keep using the exact same tool, but with different synchronization protocols.
As model behaviour, and thus the ideal synchronization protocol, might change during simulation, runtime switching of synchronization protocols is also supported.
This runtime switching can be based on performance metrics, which are logged during simulation.
Information is made available to a separate component, where a choice can be made about which synchronization protocol to use. % It's manual now, can be extended to full automatic.
Additionally, we investigate how model allocation influences the performance of our synchronization protocols.
To this end, we have included an allocation component in our simulation kernel.

Our tool is based on \textit{PythonPDEVS}~\cite{PythonPDEVS}, but implemented in C++11 for increased performance, using features from the new C++14 standard when supported by the compiler.
Unlike \textit{PythonPDEVS}, \textit{dxex} only supports multicore parallelism, thus no distributed simulation.

Using several benchmark models, we demonstrate the factors influencing the performance under a given synchronization protocol.
Additionally, we investigate a model which changes its behaviour (and corresponding ideal synchronization protocol) during simulation.
\textit{Dxex}, then, is used to compare simulation using exactly the same tool, but with a varying synchronization protocol.
With \textit{dxex} users can always opt to use the fastest protocol available, and through its modularity, users could even implement their own, or variants of existing ones.
To verify that this modularity does not hamper performance, we compare to \textit{adevs}~\cite{adevs}, another \textsf{Parallel DEVS} simulation tool.

The remainder of this paper is organized as follows:
Section~\textsc{\nameref{sec:2-background}} introduces the necessary background on synchronization protocols.
Section~\textsc{\nameref{sec:3-features}} elaborates on our design that enables the flexibility to switch protocols.
In Section~\textsc{\nameref{sec:4-performance}}, we evaluate the performance of our tool using the different synchronization protocols, and we also compare with \textit{adevs}'s performance.
We continue by introducing runtime switching of synchronization protocols and different options for model allocation in Section~\textsc{\nameref{sec:4b-hotswap}} and Section~\textsc{\nameref{sec:4a-allocation}}, respectively.
Related work is discussed in Section~\textsc{\nameref{sec:5-related-work}}.
Section~\textsc{\nameref{sec:6-conclusion}} concludes the paper and presents ideas for future work.
