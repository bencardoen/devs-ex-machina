In this section, we evaluate the performance of different synchronization protocols in \textit{dxex}.
We also compare to \textit{adevs}~\cite{adevs}, currently one of the most efficient simulation tools~\cite{DEVStoneJournal,DEVSSurvey}, to show that our modularity does not impede performance.
CPU time and memory usage is compared for different synchronization protocols.

We start with a comparison of \textit{NN} synchronization, to show how \textit{adevs} and \textit{dxex} relate in this simple case.
Afterwards, we compare inter-kernel synchronization protocols, including a comparison with \textit{adevs} again.
Inter-kernel synchronization protocols are then compared in combination with intra-kernel synchronization protocols, in the context of recent theoretical work~\cite{amdahlpdevs}.

For all benchmarks, results are well within a 5\% deviation of the average, such that only the average is used in the remainder of this section.
The same compilation flags were used for both \textit{adevs} and \textit{dxex} benchmarks (``\texttt{-O3 -flto}'').
To guarantee comparable results, no I/O was performed during benchmarks.
Before benchmarking, simulation traces were used to verify that \textit{adevs} and \textit{dxex} return exactly the same simulation results.
Benchmarks were performed using Linux, but our simulation tool works equally well on Windows and Mac.
The exact parameters for each benchmark can be found in our repository. 

The benchmarks are ran on a machine with 8 x AMD Opteron(TM) Processor 6274 with 8 cores per CPU (for a total of 64 cores) and 192 GB RAM.

\subsection{Benchmark Models}
We use three different benchmark models, covering different aspects of the simulation tool.

First, the \textit{Queue} model, based on the \textit{HI} model of DEVStone~\cite{DEVStone}, creates a chain of hierarchically nested atomic \textsf{DEVS} models.
A single generator pushes events into the queue, which get consumed by the processors after a fixed or random delay.
It takes two parameters: the width and depth of the hierarchy.
This benchmark reveals the complexity of the simulation kernel for an increasing amount of atomic models and an increasingly deep hierarchy.
An example for a width and depth of 2 is shown in Figure~\ref{fig:queue_model}.
	
Second, the \textit{Interconnect} model, a merge of PHold~\cite{PHOLD} and the \textit{HI} model of DEVStone~\cite{DEVStone}, creates $n$ atomic models, where each model has exactly one output port.
Similar to PHold, all models are connected to one another, but all through the same port: every atomic model receives each generated event (\textit{i.e.}, the event is broadcasted).
The model takes one parameter: the number of atomic models.
This benchmark shows the complexity of event creation, event routing, and simultaneous event handling.
An example for three atomic models is shown in Figure~\ref{fig:interconnect_model}.

Third, the \textit{PHold} model~\cite{PHOLD}, creates $n$ atomic models, where each atomic model has exactly $n-1$ output ports.
Each atomic model is directly connected to every other atomic model.
After a random delay, an atomic model sends out an event to a randomly selected output port.
Output port selection happens in two phases: first it is decided whether the event should be sent within the same kernel, or outside of the kernel.
Afterwards, a uniform selection is made between the possible atomic models.
The model takes two parameters: the percentage of remote events (determining the percentage of events routed to other kernels), and the percentage of high-priority events.
High-priority events are events generated in a very short time after the previous event.
This benchmark shows how the simulation kernel behaves in the presence of many local or remote events, in combination with a varying percentage of high-priority events.
An example for four atomic models, split over two kernels, is shown in Figure~\ref{fig:PHold_model}.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/queue_model_fixed.pdf}
	\caption{Queue model for depth and width 2.}
	\label{fig:queue_model}
\end{figure}
	
\begin{figure}
    \center
	\includegraphics[width=\modelfraction\columnwidth]{fig/interconnect_model.pdf}
	\caption{Interconnect model for three models.}
	\label{fig:interconnect_model}
\end{figure}

\begin{figure}
    \center
	\includegraphics[width=\modelfraction\columnwidth]{fig/phold_model.pdf}
	\caption{PHold model for four models. Color denotes the two nodes.}
	\label{fig:PHold_model}
\end{figure}

\subsection{Single kernel (\textit{NN} Synchronization)}
We start by evaluating \textit{NN} synchronization performance, in order to obtain a baseline for our comparison of other synchronization protocols.

\subsubsection{Queue}
\label{4-seq-Queue}
For the first benchmark, we tested the effect of hierarchical complexity of the model in the performance of the simulator.
A set of three tests was performed, where each test has the same number of atomic models but an increasing depth.
The results can be seen in Figure~\ref{fig:Queue_benchmark_seq}.
Since \textit{dxex} performs direct connection~\cite{SymbolicFlattening} on the model, there is no performance hit when the depth is increased.
Direct connection only needs to be done at initialization, making it a neglible one time cost for long running simulations.
\textit{Adevs}, on the other hand, suffers from the increased depth, even though some similar (but not identical) optimization to event passing was made~\cite{adevs_opt}.
With every new hierarchical layer, routing an event from one atomic model to the next becomes more expensive, resulting in an increase in runtime.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/queue_fixed_sequential.eps}
	\caption{Queue benchmark results with \textit{NN} synchronization.}
	\label{fig:Queue_benchmark_seq}
\end{figure}

\subsubsection{Interconnect}
\label{4-seq-Interconnect}
In the Interconnect model, we increase the number of atomic models, quadratically increasing the number of couplings and the number of external transitions.
As shown in Figure~\ref{fig:Interconnect_benchmark}, \textit{adevs} now outperforms \textit{dxex} by a fair margin.
Analysis showed that this is caused by the high amount of events: event creation is much slower in \textit{dxex} than it is in \textit{adevs}, despite \textit{dxex}'s use of memory pools.
To shield the user from threading and deallocation concerns, \textit{dxex} provides an event superclass from which the user can derive to create a specialized event type.
Copying, deallocation, and tracing are done at runtime, adding significant overhead when events happen frequently.
Profiling the benchmarks revealed the increased cost of output generation and deallocation as the determining factor.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/interconnect_sequential.eps}
	\caption{Interconnect benchmark results with \textit{NN} synchronization.}
	\label{fig:Interconnect_benchmark}
\end{figure}

\subsubsection{PHold}
\label{4-seq-PHold}
The PHold model is very similar to the Interconnect model.
The biggest difference is that the amount of messages sent is much lower.
The number of events scales linear with the number of atomic models, not quadratic.
Figure~\ref{fig:Phold_benchmark} shows that in terms of performance \textit{dxex} and \textit{adevs} are very close to each other, with \textit{adevs} slightly outperforming \textit{dxex}.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/phold_sequential.eps}
	\caption{PHold benchmark results with \textit{NN} synchronization.}
	\label{fig:Phold_benchmark}
\end{figure}

\subsection{Inter-Kernel Parallelism (\textit{CN} and \textit{ON} synchronization)}
We now continue by describing our inter-kernel parallelims performance for different synchronization protocols, compared to \textit{adevs}.
The speedup of \textit{adevs} is computed with the corresponding \textit{dxex} \textit{NN} synchronization benchmark.
This was done to take into account the performance difference observed in \textit{NN} synchronization.
As such, the highest speedup indicates the fastest results among all tools, independent of \textit{NN} synchronization results for \textit{adevs}.
We only compare \textit{xN} results, as we are now only interested in the differences between the various inter-kernel synchronization protocols, and have thus disabled all other forms of parallelism.
These results are generalized to conservative and optimistic synchronization in general.
The comparison between \textit{xN} and \textit{xP} is made later on.

\subsubsection{Queue}
The Queue model is one single chain of atomic models, resembling a pipeline.
This structure can be exploited to prevent cyclic dependencies in \textit{CN} and \textit{ON} synchronization.

Figure~\ref{fig:Queue_plot_strong} shows the speedup compared to \textit{NN} synchronization for a fixed problem size, with an increasing number of used CPU cores (\textit{i.e.}, strong scaling).
As the number of cores increases, \textit{ON} quickly becomes the worst choice.
This is mainly caused by the pipeline structure of the model: the last atomic models in the queue only respond to incoming messages and therefore have to be rolled back frequently.
The difference between \textit{dxex} \textit{CN} and \textit{adevs} \textit{CN} becomes smaller when more and more cores are used.
The same effect can be seen when the problem size is increased in tandem with the number of used CPU cores (\textit{i.e.}, weak scaling) in Figure~\ref{fig:Queue_plot_weak}.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/queue_fixed_strong_speedup.eps}
	\caption{Queue model strong scaling speedup compared to \textit{dxex} \textit{NN} synchronization.
             Each kernel uses one thread, and has one physical CPU core allocated to it.}
	\label{fig:Queue_plot_strong}
\end{figure}

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/queue_fixed_weak_speedup.eps}
	\caption{Queue model weak scaling speedup compared to \textit{dxex} \textit{NN} synchronization.
             Each kernel uses one thread, and has one physical CPU core allocated to it.}
	\label{fig:Queue_plot_weak}
\end{figure}
	
\subsubsection{Interconnect}
\label{subsec:parallelinterconnect}
In the Interconnect model, we determine how broadcast communication is supported across kernels.
The number of atomic models is now kept constant at eight.
Results are shown in Figure~\ref{fig:interconnect_benchmark_parallel}.
When the number of kernels increases, performance decreases due to increasing contention in conservative simulation and the increasing number of rollbacks in optimistic simulation.
All atomic models depend on each other and have no computational load whatsoever, negating any possible performance gain by splitting up the work over multiple kernels.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/interconnect_parallel.eps}
	\caption{Interconnect benchmark results for \textit{ON} and \textit{CN} synchronization, compared to \textit{dxex} \textit{NN} synchronization.
             Each kernel uses one thread, and has one physical CPU core allocated to it.}
	\label{fig:interconnect_benchmark_parallel}
\end{figure}

\subsubsection{PHold}
In the PHold model, we first investigate the influence of the percentage of remote events on the speedup.
A remote event in this context is an event that is sent from an atomic model on one kernel to an atomic model on another kernel.
When remote events are rare, optimistic synchronization rarely has to roll back, thus increasing performance.
With more frequent remote events, however, optimistic synchronization quickly slows down due to frequent rollbacks.
Conservative synchronization, on the other hand, is mostly unconcerned with the number of remote events: the mere fact that a remote event can happen, causes it to block and wait.
Even though a single synchronization protocol is ideal throughout the whole simulation run, it shows that different synchronization protocols respond very differently to a changing model.

\textit{Adevs} is significantly slower for \textit{CN} synchronization.
Analysis of profiling callgraphs shows that exception handling in \textit{adevs} is the main cause. 
To keep the models equivalent, the \textit{adevs} version does not provide the \{begin,end\}Lookahead methods, which accounts for the exception handling.
These functions require the user to implement a state saving method.
But in contrast to \textit{PythonPDEVS} and \textit{dxex}, which handle this inside the kernel, users need to manually define this.
We feel this would lead to an unfair comparison as we would like to keep the models agnostic of the underlying protocols across all benchmarks.

\begin{figure}
    \center
    \includegraphics[width=\columnwidth]{fig/phold_remotes.eps}
    \caption{PHold benchmark results using four kernels, with varying percentage of remote events.}
\end{figure}

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/phold_priority.eps}
	\caption{PHold benchmark results using four kernels, with varying amount of high-priority events.}
	\label{fig:phold_priority}
\end{figure}

Contrary to normal events, high-priority events happen almost instantaneously, restricting lookahead to a very small value.
Even when normal events occur most often, conservative synchronization always blocks until it can make guarantees.
Optimistic synchronization, however, simply goes forward in simulation time and rolls back when these high-priority events happen.
This situation closely mimics the model typically used for comparing conservative and optimistic synchronization~\cite{FujimotoBook}.

Figure~\ref{fig:phold_priority} shows how simulation performance is influenced by the fraction of these high-priority events.
If barely any high-priority events occur, conservative synchronization is penalized due to its excessive blocking, which often turned out to be unnecessary.
When many high-priority events occur, optimistic synchronization is penalized due to its unconditional progression of simulation, which frequently needs to be rolled back.
Results show that there is no single perfect synchronization algorithm for this kind of model: depending on configuration, either synchronization protocol might be better.

\subsection{Intra-Kernel Parallelism (\textit{xP} synchronization)}
The abstract simulator of \textsf{Parallel DEVS} included its own notion of parallelism, which we refered to as \pSim, and have implemented as \textit{xP} synchronization.
The \textit{xP} synchronization protocol is trivial, as it merely executes a set of independent functions concurrently.
As indicated before, intra-kernel synchronization is independent of inter-kernel synchronization in \textit{dxex}.
That is, all six combinations are possible.
Following the theoretical analysis published in~\cite{amdahlpdevs}, a comparison is warranted between \textit{xN} and \textit{xP} synchronization.

\paragraph{Model}
We have opted to use the \textit{Queue} model for this comparison, as it allows for many interesting configurations.
We have three different configurations, each using 16 threads in total, to be ran on a CPU with 16 CPU cores:
\textit{NP} synchronization uses one kernel with 16 threads each;
\textit{OP} and \textit{CP} synchronization use 4 kernels with 4 threads each; and
\textit{ON} and \textit{CN} synchronization use 16 kernels with 1 thread each.
Additionaly, \textit{NN} synchronization only uses one kernel and one thread, as it is completely sequential.
All speedup results are shown in comparison to \textit{NN} synchronization.
Atomic models are always equally distributed over the available kernels.

This allows us to observe which is more efficient in obtaining a speedup, each with the same number of CPU cores available.
We simulate a computational load by a sleep of 5 milliseconds.
The model is configured with depth 4 and width 300 if the transition function has no load (\textit{i.e.}, no sleep), and width 50 if the computational load is active (\textit{i.e.}, sleeps for 5 milliseconds).
In our configuration, an imminent atomic model always generates output, resulting in the receiving atomic model becoming imminent.
The probability that an internal transition in an atomic model generates output and is connected to a receiving atomic model is $1$ ($q = 1$~\cite{amdahlpdevs}).
The probability that an atomic model becomes immediately imminent depends on whether fixed or random time advance is used ($p$~\cite{amdahlpdevs}).
In the case of a fixed time advance, a model will always become immediately imminent ($p = 1$).
When random time advance is used, this probability is $\frac{1}{n}$, with $n$ denoting the total amount of atomic models, as only one atomic model becomes active ($p = \frac{1}{n}$).

The benchmarks are run sufficiently long enough to guarantee that the frequency of internal and external transitions is equal within a benchmark, regardless of the randomness of the time advance.
In our model each atomic model executes internal and external transitions, creating an ideal use case to evaluate the speedup \textit{xP} synchronization can obtain. 
The key difference is that although the event frequency is the same, their concurrency is not. 
The coefficient of variation of our results is less than $1\%$.
The communication overhead is hard to estimate, but given our coefficient of variation, we can expect this overhead to be constant.

We consider two cases: one where all transition functions happen simultaneously, and one where transition functions never happen simultaneously.
We defer the discussion on which of these two is the most realistic, as this depends on the problem domain.
For example, in a simulation with a fixed timestep (\textit{e.g.}, cell-based models, discretization of continuous model), transition functions often occur simultaneously.
Conversely, simulations with an arbitrary timestep (\textit{e.g.}, many independent systems communicating together) have very few simultaneous events.

\paragraph{Concurrent events ($q = 1; p = 1$)}
First we create a model where all transitions happen simultaneously, with a significant computational load in the transition functions.
In Figure~\ref{fig:psim_plot_fixed_sleep}, we observe that all synchronization protocols result in a speedup of about 10.  
In this scenario there is no real advantage between the different parallel configurations.
Note, however, that \textit{NP} synchronization is trivial to implement, whereas \textit{Ox} and \textit{Cx} synchronization are much more difficult to implement.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/pdevs_fixed_sleep.eps}
	\caption{Queue speedup benchmark with $p = 1$, $q = 1$, and significant computational load}
	\label{fig:psim_plot_fixed_sleep}
\end{figure}

\paragraph{Random events ($q = 1; p = \frac{1}{n}$)}
Now we randomize the time advance in the atomic models, resulting in very few transition functions that happen simultaneously.
Even when two transitions are only minimally apart in simulated time, they cannot be executed in parallel, as there might otherwise be a causality error.
The transition function has the same computational load as in the previous configuration. 
Results are shown in Figure~\ref{fig:psim_plot_random_sleep}.
We observe that \textit{NP} synchronization adds little overhead, but doesn't achieve any significant speedup either.
With \textit{ON} and \textit{CN} synchronization, we again achieve high speedups.
This is not the case with \textit{OP} and \textit{CP} synchronization, as we only have four kernels available for inter-kernel synchronization.
The four threads per kernel, in this case, are not used fully as only two transitions occur simultaneously at all times.

\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/pdevs_random_sleep.eps}
	\caption{Queue speedup benchmark with $p = \frac{1}{n}$, $q = 1$, and significant computational load}
	\label{fig:psim_plot_random_sleep}
\end{figure}

\paragraph{Computational load}
Finally, we remove the computation load from the transition function, in combination with many concurrent events.
Figure~\ref{fig:psim_plot_no_sleep} shows that for \textit{NP} synchronization, the overhead of thread management and shared memory communication is crippling for performance.
Even though many events occur concurrently, the computation in the transition function does not outweigh the overhead.
This results in much slower simulation than \textit{NN} synchronization.
Even \textit{OP} and \textit{CP} synchronization are unable to achieve any performance increase, whereas \textit{ON} and \textit{CN} synchronization increase performance marginally.
This is because the overhead of thread management is avoided, similar to the results obtained by Himmelspach \textit{et al.}~\cite{Himmelspach}
 
\begin{figure}
	\center
	\includegraphics[width=\columnwidth]{fig/pdevs_no_sleep.eps}
	\caption{Queue speedup benchmark with $p = 1$, $q = 1$, and trivial computational load}
	\label{fig:psim_plot_no_sleep}
\end{figure}

\paragraph{Discussion}
In \textit{dxex}, any inter-kernel synchronization protocol can be combined with any intra-kernel synchronization protocol.
While \textit{xP} synchronization can offer a significant speedup at a trivial implementation cost, $p$ must be high throughout the whole simulation.
Additionally, the computational load of the transition functions should be high enough to warrant the thread management overhead.
We conclude that each synchronization protocol has its distinct advantages and disadvantages:
inter-kernel synchronization protocols depend on the coupling of models and are difficult to implement, whereas intra-kernel synchronization protocols depend on the number of concurrent transitions and are trivial to implement.

\subsection{Memory Usage}
Apart from simulation execution time, memory usage during simulation is also of great importance.
Having insufficient memory may cause sudden deterioration in performance, even to the point of making the simulation infeasible.
We therefore also investigate the memory usage of different synchronization protocols, again comparing to \textit{adevs}.

We do not tackle the problem of states that become too large for a single machine to hold.
This problem can be mitigated by distributing the state over multiple machines, which neither \textit{dxex} nor \textit{adevs} support.

\subsubsection{Remarks}
Both \textit{dxex} and \textit{adevs} use \textit{tcmalloc} as memory allocator, allowing for thread-local allocation.
Additionally, dxex uses memory pools to further reduce the frequency of expensive system calls (\textit{e.g.}, malloc and free).
\textit{tcmalloc} only gradually releases memory back to the OS, whereas our pools will not do so at all.
Due to our motivation for memory usage analysis, we will only measure peak allocation in maximum resident set size as reported by the OS.
We only show results for \textit{xN} synchronization, as \textit{xP} synchronization has no significant additional memory requirements for a reasonable number of threads.

\subsubsection{Results}
Figure~\ref{fig:memory} shows the memory used by the different benchmarks.
Results are in megabytes, and show the total memory footprint of the running application (\textit{i.e.}, text, stack, and heap).
Note the logarithmic scale due to the high memory consumption of optimistic synchronization.

Unsurprisingly, \textit{Ox} synchronization results show very high memory usage due to the saved states.
Note the logarithmic scale that was used for this reason.
Optimistic synchronization results vary heavily depending on thread scheduling by the operating system, as this influences the drift between nodes. 
Comparing similar approaches, we notice that \textit{dxex} and \textit{adevs} have very similar memory use.

\textit{Cx} synchronization always uses more memory than \textit{Nx} synchronization, as is to be expected.
Additional memory is required for the multiple kernels, but also to store all events that are processed simultaneously.

\begin{figure}
    \includegraphics[width=\columnwidth]{fig/memory_voorlopig.eps}
    \caption{Memory usage results.}
    \label{fig:memory}
\end{figure}

\subsection{Conclusions on Performance Evaluation}
We have shown that our contribution is invaluable for high performance simulation: depending on the expected behaviour, modellers can choose the most appropriate synchronization protocol.
Each synchronization protocol has its own specific kind of models for which it is the best one.
But even with the right synchronization protocol, we have seen that two problems remain.

First, although a synchronization protocols might be ideally suited for specific model behaviour, nothing guarantees that the model will exhibit the same behaviour throughout the simulation.
Similarly to the polymorphic scheduler~\cite{MasterThesis}, we wish to make it possible for the ideal synchronization protocol to be switched during simulation.
When changes to the model behaviour are noticed, the used synchronization protocol can be switched as well. 

Second, the allocation of models is nontrivial and has a significant impact on performance.
While speedup for the Queue model, for example, was rather high in most cases, this is mostly due to characteristics of the model: the dependency graph does not contain any cycles.
When cycles were introduced, as in the Interconnect model, performance became disastrous.

In the next two sections, we elaborate on these two problems.
