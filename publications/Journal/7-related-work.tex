Several similar \textsf{DEVS} simulation tools have already been implemented, though they differ in several key aspects.
We discuss several dimensions of related work, as we try to compromise between different tools.

In terms of code design and philosophy, \textit{dxex} is most related to \textit{PythonPDEVS}~\cite{PythonPDEVS}.
Performance of \textit{PythonPDEVS} was still decent through the use of ``hints'' from the modeler.
In this spirit, we offer users the possibility to choose between different synchronization protocols.
This allows users to choose the most appropriate synchronization protocol, depending on the model.
Contrary to \textit{PythonPDEVS}, however, \textit{dxex} doesn't support distributed simulation~\cite{JDF}, model migrations~\cite{PythonPDEVS2}, or activity hints~\cite{PythonPDEVS_ACTIMS}.

Although \textit{PythonPDEVS} offers very fast turnaround cycles, simulation performance was easily outdone by compiled simulation kernels.
In terms of performance, \textit{adevs}~\cite{adevs} offered much faster simulation, at the cost of a significant compilation time.
The turnaround cycle in \textit{adevs} is much slower though, specifically because the complete simulation kernel is implemented using templates in header files.
As a result, the complete simulation kernel has to be compiled again every time.
Similarly to \textit{vle}~\cite{vle} and \textit{PowerDEVS}~\cite{PowerDEVS}, \textit{dxex} compromises by separating the simulation kernel into a shared library.
After the initial compilation of the simulation tool, only the model has to be compiled and linked to the shared library.
This significantly shortens the turnaround cycle, while still offering good performance.
In terms of performance, \textit{dxex} is shown to be competitive with \textit{adevs}.
Despite its high performance, \textit{adevs} does not support optimistic synchronization, which we have shown to be highly relevant.

Previous \textsf{DEVS} simulation tools have already implemented multiple synchronization protocols, though none have done it in a strictly modular way that allows straightforward protocol switching for a single given model.
For example, \textit{adevs} only supports conservative synchronization, and \textit{vle} only supports experiment-level parallelism (\textit{i.e.}, run multiple experiments concurrently).
Closest to our support for multiple synchronization protocols is \textit{CD++}~\cite{CD++}.
For \textit{CD++}, both a conservative (\textit{CCD++}~\cite{CCD++}) and optimistic (\textit{PCD++})~\cite{PCD++}) variant exist.
Despite the implementation of both protocols, they are different projects entirely.
Some features might therefore be implemented in \textit{CCD++} and not in \textit{PCD++}, or vice versa.
And while this might not yet be that significant a problem to this day, this problem will only get worse when each project follows its own course.
\textit{Dxex}, on the other hand, is a single project, where the choice of synchronization protocol is a simple configuration.
\textit{CD++}, however, implements both conservative and optimistic synchronization for distributed simulation, whereas we limit ourselves to parallel simulation. 
A new architecture for sequential PDEVS simulation has been introduced in \cite{SeqPDEVSArch} with promising performance results. This algorithm achieves a speedup by using the inherent parallelism in PDEVS without the need for synchronization.
By limiting our approach to a pure parallel simulation, we are able to achieve higher speedups through the use of shared memory communication. 
%TODO Explain complementary nature pdevs/sync + remove or link seq pdevs architecture

In the \textsf{PDES} community, the problem of choosing between synchronization protocols is well known and documented~\cite{Jha:1994:UFC:195291.182480}.
The challenges of implementing such runtime switching have previously been explored already~\cite{Das:1996:APP:256562.256602}, and an implementation was given by, for example~\cite{perumalla2005musik}.
Our contribution entails bringing this same feature to the \textsf{Parallel DEVS} community, further expanding upon our support for multiple synchronization protocols.

Model allocation and its impact on parallel performance has previously also been studied in the \textsf{PDES} community~\cite{PDESpartitioning}.
Referenced as partitioning of the simulation model, most studies distinguish between communication overhead and computational distribution (load balancing) as the two dimensions to partition over.
Partitioning a simulation model is identified as an issue to achieve scalability~\cite{Scalability}. 
Some research in the context of \textsf{Parallel DEVS} has been done, where they turn their attention to load balancing and communication overhead~\cite{PDEVSpartitioning, NonFragmenting}.
Our contribution studies the effect of partitioning with emphasis on the effect of communication between processes and in the presence of a flattened hierarchy. 
We focus on static partitioning since this is a limiting factor for our conservative synchronization implementation which does not support model relocation.
Model relocation, as implemented by \textit{PythonPDEVS}~\cite{PythonPDEVS2}, might be an interesting addition to only model allocation at the start of simulation.

In summary, \textit{dxex} tries to find the middle ground between the concepts of \textit{PythonPDEVS}, the performance of \textit{adevs}, and the multiple synchronization protocols of \textit{CD++}.
To further profit from our multiple synchronization protocols in a single tool, we further added runtime switching between synchronization protocols and model allocation support.
