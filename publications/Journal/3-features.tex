Historically, \textit{dxex} is based on \textit{PythonPDEVS}~\cite{PythonPDEVS}.
Python is a good language for prototypes, but performance has proven insufficient to compete with C++-based simulation kernels~\cite{MasterThesis}.
\textit{Dxex} is a C++11-based \textsf{Parallel DEVS} simulator, based on the design of \textit{PythonPDEVS}.
Whereas the feature set is not too comparable, the architectural design, simulation algorithms, and optimizations are highly similar.

We will not make a detailed comparison with \textit{PythonPDEVS} here, but only mention some supported features.
\textit{Dxex} supports, similarly to \textit{PythonPDEVS}, the following features: direct connection~\cite{SymbolicFlattening}, \textsf{Dynamic Structure DEVS}~\cite{DSDEVS}, termination conditions~\cite{JDF}, and a modular tracing and scheduling framework~\cite{PythonPDEVS}.
We do not elaborate on these features in this paper.
But whereas \textit{PythonPDEVS} only supports optimistic synchronization, \textit{dxex} support multiple synchronization protocols (though not distributed).
This is in line with the design principle of \textit{PythonPDEVS}: allow users to pass performance hints~\cite{PythonPDEVS_ACTIMS} to the simulation kernel.
In our case, a user can configure the simulation kernel with the synchronization protocol to use, or even switch the synchronization protocol during runtime.
Our implementation in C++11 also allows for (compiled) optimizations which were plainly impossible in an interpreted language, such as Python.
\textit{Dxex} uses new optimizations from the C++14 standard when possible.
The C++11 standard threading primitives are used to run the different simulation kernels.
Within a single simulation kernel, OpenMP~\cite{openmp4} is used to parallelize the transition functions, as is usual in \textsf{Parallel DEVS}.

Since there is no universal \textsf{DEVS} model standard, \textit{dxex} models are incompatible with \textit{PythonPDEVS} and vice versa.
This is due to \textit{dxex} models being grafted on C++11, whereas \textit{PythonPDEVS} models are grafted on Python.

In the remainder of this section, we elaborate on our prominent new feature: the efficient implementation of multiple synchronization protocols within the same simulation tool, offered transparently to the model.

\subsection{Synchronization protocols}
We previously explained the existence of different synchronization protocols, each optimized for a specific kind of model.
As no single synchronization protocol is ideal for all domains, a general purpose simulation tool should support multiple protocols.
We argue that a general purpose simulation tool should support all six standard synchronization protocols, as is the case for \textit{dxex}.

Different protocols relate to different model characteristics.
For example, \textit{Cx} is for when high lookahead exists between different nodes, whereas \textit{Ox} is for when lookahead is unpredictable.
It is possible for the synchronization overhead to become larger than the achieved parallelism, resulting in slower simulation than fully sequential execution (\textit{NN}).

Data exchange between different simulation kernels happens through shared memory, using the new C++11 synchronization primitives.
This was also possible in previous versions of the C++ standard by falling back to non-portable C functions.
C++11 further allows us to make the implementation portable, as well as more efficient: the compiler makes further optimizations to heavily used components.

\subsubsection{Inter-Kernel Synchronization}
Our core simulation algorithm is very similar to the one found in \textit{PythonPDEVS}, including many optimizations.
Minor modifications were made though, such that it can be overloaded by different synchronization protocol implementations.
This way, the \textsf{DEVS} simulation algorithm is implemented once, but parts can be overridden as needed.

An overview of \textit{dxex}'s design is given in Figure~\ref{fig:class_diagram}.
It shows that there is a simulation \texttt{Kernel}, which simulates the \texttt{AtomicModel}s connected to it.
The superclass \texttt{Kernel} represents a standalone simulation kernel (\textit{Nx}).
Subclasses define specific variants, such as \texttt{ConservativeKernel} (conservative synchronization), \texttt{OptimisticKernel} (optimistic synchronization), and \texttt{DynamicKernel} (\textsf{Dynamic Structure DEVS}).
In theory, more synchronization protocols (\textit{e.g.}, other algorithms for conservative synchronization) can be added without altering our design.

\begin{figure*}
    \includegraphics[width=\textwidth]{fig/cores_class_diagram.eps}
	\caption{\textit{Dxex} design.}
	\label{fig:class_diagram}
\end{figure*}

The following inter-kernel synchronization protocols are implemented.

\paragraph{None (\textit{Nx})}
No inter-kernel synchronization is the base case, implemented in the \texttt{Kernel}.
It can be overloaded by any of the other simulation kernels, which augment it with inter-kernel synchronization methods.

\paragraph{Conservative (\textit{Cx})}
For conservative synchronization, each kernel determines the kernels it is influenced by.
Each model needs to provide a lookahead function, which determines the lookahead depending on the current simulation state.
Within the returned time interval, the model promises not to raise an event.

\paragraph{Optimistic (\textit{Ox})}
For optimistic synchronization, each node must be able to roll back to a previous point in time.
This is implemented with state saving.
This needs to be done carefully in order to avoid unnecessary copies, and minimize the overhead.
We use the default: explicitly save each and every intermediate state.
Mattern's algorithm~\cite{mattern} is used to determine the GVT, as it runs asynchronously and uses only $2n$ synchronization messages.
Once the GVT is found, all nodes are informed of the new value, after which fossil collection is performed, and irreversible actions (\textit{e.g.}, printing) are committed.
The main problem we encountered in our implementation is the aggressive use of memory.
Frequent memory allocation and deallocation caused significant overheads, certainly when multiple threads do so concurrently.
This made us switch to the use of thread-local %(using \textit{tcmalloc})% see page 6 for introducing tcmalloc
 memory pools.
Again, we made use of specific new features of C++11, that are not available in Python, or even previous versions of the C++ language standard.

\subsubsection{Intra-Kernel Synchronization}
In our tool, each simulation kernel is capable of executing concurrent transitions in parallel, whether they are external, internal, or confluent.

\paragraph{None (\textit{xN})}
No intra-kernel parallelism is used, meaning that all concurrent transitions are processed sequentially.
Note that the order in which they are processed is non-deterministic.

\paragraph{\pSim (\textit{xP})}
With intra-kernel parallelism, a configurable number of threads is allocated to optimally divide the processing load of concurrent transitions.
The parallel execution of transitions introduces some overhead in thread pooling and locking, and disallows some optimizations in optimally rescheduling models.
The concurrency of events and the combined computational load of the transitions has to outweigh this overhead to obtain a significant speedup, as we will demonstrate in Section~\textsc{\nameref{sec:4-performance}}.

\subsection{Synchronization Protocol Transparency}
We define synchronization protocol transparency as having a single model, which can be executed on any kind of kernel, without any modifications.
User should thus only provide one model, implemented in C++11, which can be simulated using any of the six synchronization protocols.
The synchronization protocol to use is a simple configuration option.
The exception is conservative synchronization (\textit{Cx}), where a lookahead function is required, which is not used in other synchronization kernels.
Two options are possible: either a lookahead function must always be provided, even when it is not required and possibly not used, or we use a default lookahead function if none is defined.

Always defining a lookahead function might seem redundant, especially should the user never want to use conservative synchronization.
Defining the lookahead is difficult and can often be unpredictable.
The more attractive option is for the simulation tool to provide a default lookahead function, such that simulation can run anyway, but likely not at peak performance.
Depending on the model, simulation performance might still be faster than \textit{Nx}. 

Defining a lookahead function is therefore recommended in combination with conservative synchronization, but it is not a necessity, as a default value $\epsilon$ (\textit{i.e.}, the smallest representable timestep) is used otherwise.
Providing this default implementation has no impact in sequential or optimistic simulation; as the function is never called, the compiler will optimize it out.
By providing this default implementation in the model base class we ensure that a model can run with any synchronization protocol.

\subsection{Increasing Parallelism}
The goal of our contribution is to increase simulation performance as much as possible, leveraging parallel computation in the process.
Parallelizing goes further, however, than merely implementing the different synchronization protocols.

We observed that after implementing all synchronization protocols, performance was still not within acceptable levels.
Profiling revealed that most of the overhead was caused by two issues: memory management and random number generation.
For both, it is already known that they can have significantly impact on parallelizability of code, since they introduce sequential blocks.
Both were tackled using approaches that are in common use in the parallel programming world.
We briefly mention how the application of these techniques influences our performance.

\subsubsection{Memory Management}
\label{sec:4-subsec:overhead-pgraph:memory}
Memory management is traditionally seen as one of the major bottlenecks in parallel computation~\cite{Memory}, since memory bandwidth doesn't increase as fast as the number of CPU cores using it.
While this is always a problem, it is aggravated in \textit{dxex} by providing automatic memory management for events and states.
A model written for \textit{Nx} synchronization will run correctly with conservative (\textit{Cx}) or optimistic (\textit{Ox}) synchronization without altering, from the point of view of the modeller, the (de)allocation semantics of events or states.

Furthermore, allocating and deallocating memory by making calls to the operating system, as is typically done by calls such as \texttt{malloc}, happens sequentially.
To counter this, our memory allocators are backed by a thread-aware pooling library.
With \textit{Nx} synchronization, no allocated event persists beyond a single time advance, even allowing the use of an arena-style allocator.
Conservative and optimistic simulation need to use generic pool allocators since events are shared across kernels and thus have a different lifetime.

Intra-kernel events can be (de)allocated without synchronization with the other kernels. 
They can be returned immediately to the memory allocator as the lifetime of these objects is known at creation. 
In contrast, inter-kernel events need a GVT algorithm to determine when safe deallocation can occur.
Intra-kernel synchronization protocols therefore have a lower overhead than inter-kernel synchronization protocols.
Simulations with many inter-kernel events suffer a performance hit, whereas the impact of many intra-kernel events can be minimized using arena allocators~\cite{Arena}.

\textit{Dxex} uses \textit{Boost Pool}~\cite{boostpool} allocators for \textit{Cx} and \textit{Ox} synchronization, and arena-style allocators for \textit{Nx} synchronization.
The latter can be faster, but at the cost of additional configuration.
The allocators are supplemented by the library \textit{tcmalloc}~\cite{tcmalloc}, reducing lock contention in \texttt{malloc} calls.

\begin{figure}
    \center
    \includegraphics[width=\columnwidth]{fig/memory_allocators_parallel.eps}
    \caption{Effect of memory allocators with \textit{ON} synchronization.}
    \label{fig:memallocators_parallel}
\end{figure}

We primarily investigate this for optimistic simulation, as this is the most memory consuming mode of simulation~\cite{FujimotoBook}.
Simulation execution times for all four combinations are shown in Figure~\ref{fig:memallocators_parallel}.
Optimistic simulation greatly benefits from the use of \textit{tcmalloc}, regardless of the allocator.
Nonetheless the pool allocator also reduces the allocation overhead, though only by a relatively small fraction.
Both techniques are required to reduce the overhead of memory allocations in \textit{dxex}, and are turned on by default.

Both pools and \textit{tcmalloc} try to keep memory allocated instead of returning it to the Operating System (OS).
As a result, the OS will usually report memory consumption that is higher than the actual amount of stored data.

\subsubsection{Random Number Generators}
Random Number Generators (RNG) are another aspect of the program that can limit parallelization.
All accesses to the RNG will result in the modification of a global (\textit{i.e.}, shared between threads) variable.
This easily becomes a bottleneck in simulation, since random numbers are a common occurrence in simulation~\cite{Random}.
As such, a nontrivial amount of time in a simulation is often spent waiting for an RNG.

We still need to guarantee determinism and isolation between the calls to the RNG, as well as avoiding excessive synchronization.
\textit{Dxex} uses the Tina RNG collection (TRNG)~\cite{PhysRevE.75.066701} as an alternative random number generator with performance and multithreading in mind.
%TODO one RNG per kernel or thread in new terminology?
Since the RNG is an implicit part of the state in the \textsf{Parallel DEVS} formalism, though often not implemented as such, we evaluated performance for both approaches: one global RNG per thread, and one RNG per atomic \textsf{DEVS} model.

We see in Figure~\ref{fig:Queuerngspeedup} that storing the RNG in the state is very expensive for the default Standard Template Library (STL) random number generator with optimistic synchronization.
This is primarily caused by the significant difference in size: 2504 bytes for the STL random number generator, and 24 bytes for the Tina random number generator.
Only \textit{Ox} synchronization seems affected, as it needs to copy more bytes in every transition due to state saving.
No additional copies need to happen in \textit{Nx} or \textit{Cx} synchronization.

Figure~\ref{fig:Queuerngspeedup} shows that, for \textit{NN} synchronization, any of the three alternatives is three times faster than STL RNG.
For \textit{Cx} and \textit{Ox} synchronization, the synchronization overhead seems to be the main bottleneck, as seen by the big speedup gap with \textit{Nx} synchronization.
\textit{Cx} synchronization is almost unaffected by changing the RNG.

\begin{figure}
    \center
    \includegraphics[width=\columnwidth]{fig/rngspeedupeffectdevstone.eps}
    %TODO: why is STL RNG in state just as fast in sequential? Does storing it in the state really do that much?
    % The pattern is identical, the only difference is in optimistic where the copying of a larger structure
    % (stl rng in state) becomes expensive enough. Another possible reason for a speedup w.r.t not storing
    % is the dependency between calls to the random number generator, the compiler can determine that
    % these calculations are local to the object.
    \caption{Speedup with different RNG usage patterns compared to STL random number generator.}
    \label{fig:Queuerngspeedup}
\end{figure}
