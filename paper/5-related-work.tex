%TODO explain about other DEVS simulation kernels, and how they differ
%TODO explain about PythonPDEVS, the most related in terms of feature and "legacy" --> now in C++11 and way better performance
\subsection{PythonPDEVS}
Dxexmachina is closely related to PythonPDEVS in design and philosophy. PythonPDEVS allows anyone who grasps the DEVS formalisms to immediately simulate his/her model without having to consider the kernel implementation. C++ implementations cannot hope to match the fast prototype/edit/run cycles provided by PythonPDEVS, although this can be minimized by building the kernels as libraries. %maybe warn that if a header is touched, you still have to compile from scratch ? (eg. TIME_FP)
Advanced features such as activity based relocation and the performance gains this results in, are still unique to PythonPDEVS.
\subsection{Adevs}
%TODO explain about adevs, the most related in terms of performance --> we implement more synchronization options and offer better performance because ...
Adevs's source code is still under active development, allowing for an exact comparison in performance and features. It remains in most aspects the fastest simulation engine for the DEVS formalism, but it lacks an optimistic synchronization implementation. %Tracing ?
By virtue of not flattening Coupled Models, performance suffers in increasingly hierarchical models.
%checkme
\subsection{CD++}
%TODO explain about CD++, the most related in terms of synchronization --> we implement everything in a single program, with a non-fragmented code-base
%TODO make mention of Warped, which is the kernel used in CD++, and why we don't use it
Different projects on CD++ offer conservative (CCD++) as well as optimistic (PCD++) parallel simulation. In contrast to our single program, with a non-fragmented code-base, neither projects offer both synchronization protocols. CD++ relies on the WARPED kernel. It is a middleware that provides memory, event, file, time and communication scheduling. WARPED is not used here since we operate explicitly on a shared memory system and since we wanted to design our kernels using the least amount of overhead possible.
