%TODO say that you will now prove your claims about the aproach having nearly no impact
%TODO and potentially even claim that you are faster than adevs for some situations, due to the algorithms from PythonPDEVS and further features
%TODO briefly give some hardware info on the machine (CPU, OS, RAM), used software (version of adevs, GCC, ...), and about number of samples/variance
\subsection{Sequential Simulation}
%TODO we start with sequential simulation results, because this is still base case
%TODO compare CPU time with adevs for some benchmarks
%TODO and explain why there is such a difference
%TODO (spend some time here, as this is also important)
In our benchmarks, we will consistently compare the performance of dxex with adevs. The implemented models for the each benchmark are functionally identical.\\
Dxex uses integer timestamps whereas adevs uses floating point time representation. Tests show there is little difference performance-wise (both types fit in a machine word).\\
The benchmarks have been compiled using the {\tt -O3} option and link-time optimization, with all IO disabled. To compare the models in details, we have run the different benchmarks with different parameters.\\
The benchmarks have been run on a i7-2670QM, 16GiB RAM, Arch Linux (4.2.5 kernel) and G++ 5.2. Interpretation of the benchmark results is done using call-graphs generated by perf. A selection of those call-graphs is available in the code repository. For the runtime plots, perf is used in combination with R.\\
%Note that a fixed timeadvance (e.g. devstone) is set at 100.
\subsubsection{Devstone}
The Devstone \cite{DEVStone} benchmark is highly hierarchical. As can be seen in Figure~\ref{fig:Devstone}, dxex is outperforming adevs considerably. This can be explained by the fact that, using Direct connection, the hierharchical structure is being flattened whereas, in the adevs case, considerable overhead is caused by traversing the coupled model's structure to pass events.
\begin{figure}[h]
	\includegraphics[width=.5\textwidth]{fig/fig1.eps}
	\caption{Devstone}
	\label{fig:Devstone}
\end{figure}

\subsubsection{PHold}
PHold \cite{PHOLD} is a benchmark that focuses in particular on parallel execution; the sequential runtime is therefore only added as a baseline. A model selects the destination of a message at runtime using a state-saved RNG, which takes up a non-trivial amount of runtime. The results of the benchmark are visualized in Figure~\ref{fig:PHold}.
%% needs further clarification...
\begin{figure}[h]
	\includegraphics[width=.5\textwidth]{fig/fig4.eps}
	\caption{PHold}
	\label{fig:PHold}
\end{figure}

\subsubsection{Interconnect}
Interconnect~\cite{van2013research} is a benchmark where all models broadcast, creating a complete graph in dependencies between models. As the model count increases, we see (see Figure~\ref{fig:Interconnect} the expected quadratic increase in runtime for both adevs and dxex, but an increasing penalty for dxex. Profiling shows this is entirely due to the heap allocation of messages, which even though minimized by using memory pools remains significant.
\begin{figure}[h]
	\includegraphics[width=.5\textwidth]{fig/fig3.eps}
	\label{fig3.eps}
	\caption{Interconnect}
	\label{fig:Interconnect}
\end{figure}

\subsection{Parallel Simulation}
%TODO explain that we will now compare different synchronization protocols
%TODO compare different synchronization protocols with adevs' conservative, showing that we are faster (or at least, not that much slower) than adevs
By default, the benchmarks use 4 kernels for parallel simulation.
\subsubsection{Devstone}
The flattened models are allocated to kernels by giving each kernel a distinct section of the chain, resulting in a low ratio of inter-kernel to intra-kernel messages. For the optimistic case, this can cause more reverts since the kernels will start to drift faster as the model-count increases. Furthermore, optimistic synchronization is quite sensitive to an increase in kernels, since the delay before a revert propagates, increases. This benchmark requires a specific warm-up time; for $n=d\times w$ models, it takes $\textup{timeadvance}()*(n-1)$ transitions to activate the last model in the chain. When using parallel processing, this can be reduced to $\textup{timeadvance}()*(n\frac{kernels-1}{kernels} -1)$ before the last kernel becomes active.\\
Figure~\cite{fig:DevstonParallel} shows that dxex outperformance adevs (in both optimistic and conservative case).
\begin{figure}[ltbh]
	\includegraphics[width=.5\textwidth]{fig/fig2.eps}
	\label{fig:DevstoneParallel}
	\caption{DevStone parallel}
\end{figure}
\subsubsection{PHold}
%Phold
In Phold, the allocation is specified in the benchmark itself. Each kernel manages a single node with a constant set of sub-nodes. The parameter $R$ (which is here set to $10$) determines the percentage of remote destination models.\\
The dynamic dependency graph is a very sparse version of the static dependency graph, penalizing the conservative case. The lookahead is $\epsilon$, so the conservative case spends most of its time crawling in steps of $\epsilon$. Since the dependency graph between kernels is a complete graph, this is not a simulation that scales in our implementation. For $N$ kernels, each kernel has to query the null-time of $N-1$ kernels, resulting in O($N^2$) polling behaviour. In a non-cyclic simulation with a non-trivial lookahead (like in, e.g., Devstone), that choice does pay off (see Figure~\ref{fig:DevstoneParallel}). Adevs's lesser performance is due in part to their lookahead management, which after profiling shows to spend a non-trivial amount of time in exception handling code.\\
The optimistic case suffers little from the above problems; due to the high interconnectivity, however, a cascading revert is still possible. With the percentage of remotes equal to $100$, Phold reflects interconnect in behaviour, which is why the $R$ value is not dimensioned here. A revert is very expensive in PHold due to our usage of C++11's random nr generators. The cost of a revert is dominated by the recalculation of destination models, not in allocating/deallocating states/messages. Once a revert happens the drift between kernels increases fast, increasing the likelihood of more reverts. Despite all this, optimistic can for low $R$ values quickly exploit the uncertainty that slows down conservative in this benchmark.

\subsubsection{Interconnect}
In Interconnect, the set of atomic models form a complete graph (w.r.t. connections); each model broadcasts messages to the entire set.\\
Allocation cannot avoid cycles and the resulting dependency graph between kernels remains a complete graph. The runtime dependency graph is almost immediately identical to the static graph.\\
The conservative case still shows the same issues as in PHold, with the key difference , for a fixed time advance, the lookahead is equal to the timespan between transitions. The scaling issue is identical as with PHold.\\
In this benchmark, the optimistic case runs very quickly out of memory. With $c$ kernels and $N$ atomic models, a single revert undoing $k$ transitions will lead to $(N-1)\times k$ messages that need to be recreated, plus $(N-1) \frac{c-1}{c} \times k$ anti-messages that need to be sent.

\subsubsection{Priority network model}
%TODO show a single plot which varies a parameter that changes the ideal synchronization protocol
The priority benchmark is composed of a single server generating a stream of $0\leq m \leq n$ messages at fixed time intervals, interleaved with a probability $p$ for a priority message to $n$ receivers (see Figure~\ref{fig:NetworkModel}).\\ This defaults the lookahead for the receivers to $\epsilon$, but this time there is no scaling effect, nor are there cycles in the dependency graph. This model therefore highlights the basic strengths/weaknesses of both synchronization protocols. Receiving models are allocated on another kernel than the server and have an internal transition so that they will not wait for the incoming messages.
\begin{center}
\begin{figure}
\input pqueue.tex
\caption{The priority network model}
\label{fig:NetworkModel}
\end{figure}
\end{center}
A key difference here with the other benchmarks is that a state (in the Receiver instances) is very cheap to copy/create. The kernel holding the server will never revert since it is a source in the dependency graph. The optimistic case will therefore not suffer the same performance hit in recreating the states as it does in PHold. The overhead in the optimistic case is entirely due to the factor $m$, which will quickly dominate in increasing buffers of received messages.
\begin{figure}[ltbh]
	\includegraphics[width=.5\textwidth]{fig/fig5.eps}
	\label{fig:Priority}
	\caption{Priority}
\end{figure}
Interestingly, the parameter $p$ does not clearly favour either synchronization protocol (see Figure~\ref{fig:Priority}). While this removes any possibility for a lookahead, the conservative case can quickly bridge the timespan between fixed messages since there is no cyclic dependency. Lightweight states prevent performance loss due to reverts in the optimistic case; only the overhead in event handling in the optimistic case eventually becomes the deciding factor.

%TODO (keep explaining as much as space permits, as this is the evaluation of the core contribution)
\subsection{Memory Usage}
%TODO compare memory usage with adevs for some benchmarks
\subsubsection{Platform and tools}
Both dxex and adevs use tcmalloc as memory allocator. Additionally, dxex uses memory pools to further reduce the frequency of expensive system calls (malloc/free/sbrk/mmap/...). Tcmalloc will only gradually release memory back to the OS, whereas our pools will not do so at all. If memory has been allocated once, it is from a performance point of view better to keep that memory in the pool. This is one reason why memory utilization is best measured by peak allocation. Profiling is done using Valgrind's massif tool~\cite{Nethercote:2007:VFH:1273442.1250746}.
The platform used for memory profiling has an i5-3317U Intel CPU and 8GiB RAM with a page size of 4,096KiB, running Fedora 22 (kernel 4.2.6).
\subsubsection{Measure}
Adevs passes messages by value, dxex passes a pointer. The runtime effects of this choice have already been demonstrated in the Interconnect benchmark, so, in this section, we measure memory usage in number of allocated pages combining text, stack and heap memory for the program profiled. For the OS and/or user, this is the actual memory footprint of the application. It is important to note that, especially in the optimistic case, not all this memory is always in use by the kernels. During simulation, the pools will generally not return deallocated memory to the OS, but keep it for later reuse.
\subsubsection{Results}
\paragraph*{Devstone}
\begin{table}[htb]
	\centering
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		adevs & adevs con &dxex &dxex con&dxex opt\\ \hline
		44 & 70 & 42 & 75 & 363  \\ \hline
	\end{tabular}
	\caption{Devstone 40x40 t5e5, unit MiB, 4 kernels (if parallel)}
	\label{dtone_mem}
\end{table}
Since, in the conservative case, messages are passed by pa ointer, a GVT/LBTS implementation is required to organise the garbage collection. This inevitable delay explains the higher memory usage compared to adevs.\\
%TODO next sentence needs rewriting --done?
Optimistic's TimeWarp requires state/event saving, and its GVT algorithm is more complex (with a resulting higher latency) than the LBTS calculation in the conservative case. 
Moreover, the differences in LP virtual times are far larger compared to conservative time synchronization. All these factors explain the heavier memory usage. Devstone (flattened) is allocated in a chain. Leafs in the dependency graph will therefore do a lot of unnecessary simulation before having a revert, leading to an increased memory pressure. Unlike conservative and sequential execution, memory usage in the optimistic case varies greatly depending on scheduling of kernel threads and drifting between kernels. 
\paragraph*{PHold}
\begin{table}[htb]
	\centering
	\label{dtone_mem}
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		adevs & adevs con &dxex &dxex con &dxex opt\\ \hline
		40 & x & 37 & 61 & 682  \\ \hline
	\end{tabular}
	\caption{Phold n 4 s 16 t1e6 r 10, unit MiB, 4 kernels (if parallel)}
\end{table}
With only 10\% of all messages being inter-kernel, we expect conservative to have memory consumption near that of the single threaded implementation, since intra-kernel messages are reclaimable after each round. The counterintuitive high memory usage can be explained by conservative's stalled round behaviour which occurs whenever a kernel cannot advance (eit $==$ time). In such a round messages are sent out but the kernel does not execute any transitions until it has received all input from all influencing kernels. \\
With lookahead $\epsilon$ this then leads to a high frequency of polling on shared null-times, which are used to determine lbts and thus garbage collection.
The lbts calculation will not wait until a new value is found, since this can create unwanted contention with the simulation. The resulting longer time intervals within which no new lbts is found delay memory deallocation.
Adevs' conservative fails to complete the benchmark under valgrind even with a significantly reduced load.
Optimistic exhibits the expected high memory usage.
\paragraph*{Interconnect}
In section 4.2 the parallel performance of this benchmark is further explained. Interconnect highlights dxex's lower performance due to message allocation overhead. 
\begin{table}[lhtb]
	\centering
	\label{iconn_mem}
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		adevs & adevs con &dxex &dxex con & dxex opt\\ \hline
		39 & 39 & 35 & 43 & 259 \\ \hline
	\end{tabular}
	\caption{Interconnect w 20 t5e5, unit MiB, 2 kernels (if parallel)}
\end{table}
		
\paragraph*{Priority Network}
The priority network model is detailed in section 4.1.\\
The high memory usage of the conservative case is largely due to the garbage collection implementation. Kernel 0 is the initiator of the LBTS calculation but since it holds only models independent of any other (server), it will finish simulation very fast leaving the other kernel bereft of an updated LBTS. The kernel holding the server will after simulation wait on the receiving kernel until it can prove all sent messages can be deallocated.
\begin{table}[lhtb]
	\centering
	\label{pmod_mem}
	\begin{tabular}{| l | l | l |}
		\hline
		dxex &dxex con &dxex opt\\ \hline
		35 & 450 & 651\\ \hline
	\end{tabular}
	\caption{Priority model n 128, m 16, p 10,  t2e8, unit MiB, 2 kernels}
\end{table}
